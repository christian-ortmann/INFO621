{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSMWLDo3DNi7"
      },
      "source": [
        "<h1 align=\"center\">INFO621 - Advanced Machine Learning Applications</h1>\n",
        "\n",
        "<h2 align=\"center\"><strong>Homework 3: Transformers and CNNs</strong></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PveUHb8ELkT"
      },
      "source": [
        "# Guidelines\n",
        "\n",
        "**Worth:** 10% of your final grade (**100 points total**)  \n",
        "**Submission Deadline:** Friday, March 07, 11:59 PM (Tucson Time)\n",
        "\n",
        "\n",
        "### **Instructions**\n",
        "\n",
        "- For exercises involving code, write your solutions in the provided code chunks. You may add additional code chunks if needed.  \n",
        "- For exercises requiring descriptions or interpretations, use full sentences and provide clear, concise explanations.  \n",
        "\n",
        "### **Policies**\n",
        "\n",
        "**Sharing/Reusing Code Policy:**  \n",
        "You are allowed to use online resources (e.g., RStudio Community, StackOverflow) but **must explicitly cite** any external code you use or adapt. Failure to do so will be considered plagiarism, regardless of the source.  \n",
        "\n",
        "**Late Submission Policy:**  \n",
        "- **Less than 1 day late:** -25% of available points.  \n",
        "- **1-7 days late:** -50% of available points.  \n",
        "- **7 days or more late:** No credit will be awarded, and feedback will not be provided.  \n",
        "\n",
        "**Declaration of Independent Work:**  \n",
        "You must acknowledge your submission as your independent work by including your **name** and **date** at the end of the \"Declaration of Independent Work\" section.  \n",
        "\n",
        "### **Grading**\n",
        "\n",
        "- **Total Points:** 100 points.\n",
        "\n",
        "- **Grade Breakdown:**  \n",
        "  - **Part 1 (40 Points Total):**  \n",
        "    - Multiple-Choice Questions: 5 questions, 4 points each.  \n",
        "    - Descriptive Questions: 4 questions, 5 points each.  \n",
        "\n",
        "  - **Part 2 (45 Points Total):**  \n",
        "    - 9 code completion tasks.  \n",
        "\n",
        "  - **Part 3 (15 Points Total):**  \n",
        "    - 2 code completion task.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9zdLwb4E3tN"
      },
      "source": [
        "# Part 1. Written Questions (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1WPqS4rE5Eg"
      },
      "source": [
        "- **Multiple-Choice Questions**: 5 questions, 4 points each  \n",
        "- **Descriptive Questions**: 4 questions, 5 points each"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHKyfg7VE7WM"
      },
      "source": [
        "## 1.1 Multiple-Choice Questions (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQdyMoFRFBR9"
      },
      "source": [
        "1. **Which mechanism enables Transformers to capture global dependencies among tokens? (4 points)**\n",
        "\n",
        "  - A) Convolution\n",
        "  - B) Self-Attention\n",
        "  - C) Recurrent Processing\n",
        "  - D) Max-Pooling\n",
        "  - E) Fully Connected Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC2iCKxJNowk"
      },
      "source": [
        "**Your Answer**: B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95X6LgUhNfS3"
      },
      "source": [
        "2. **What is the primary purpose of positional embeddings in Transformers? (4 points)**\n",
        "\n",
        "  - A) To reduce the dimensionality of token embeddings\n",
        "  - B) To normalize the activations\n",
        "  - C) To initialize the weight matrices\n",
        "  - D) To encode the order of the tokens\n",
        "  - E) To improve token similarity calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pok8-bIiNpV_"
      },
      "source": [
        "**Your Answer**: E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sNyHe8eNjFJ"
      },
      "source": [
        "3. **In Convolutional Neural Networks (CNNs), what advantage does the convolution operation provide? (4 points)**\n",
        "\n",
        "  - A) Global connectivity with all image pixels\n",
        "  - B) Sequential processing of image rows\n",
        "  - C) Automatic extraction of textual features\n",
        "  - D) Increased interpretability of model decisions\n",
        "  - E) Local connectivity with parameter sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAwVBr2cNpx6"
      },
      "source": [
        "**Your Answer**: E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S8j0oNaNk2f"
      },
      "source": [
        "4. **Which component in a Transformer block is responsible for introducing non-linearity after the self-attention mechanism? (4 points)**\n",
        "\n",
        "  - A) Self-Attention\n",
        "  - B) Feed-Forward Network (FNN)\n",
        "  - C) Positional Encoding\n",
        "  - D) Residual Connection\n",
        "  - E) Layer Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT4J6rS9NqML"
      },
      "source": [
        "**Your Answer**: B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h31J2BoNmNn"
      },
      "source": [
        "5. **What is the role of residual connections in deep Transformer architectures? (4 points)**\n",
        "\n",
        "  - A) They help alleviate vanishing gradients and ease training\n",
        "  - B) They add extra parameters to the model\n",
        "  - C) They perform dimensionality reduction\n",
        "  - D) They serve as pooling operations\n",
        "  - E) They replace the need for self-attention layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EicGZ9kmNqly"
      },
      "source": [
        "**Your Answer**: A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC6UaogEO9xV"
      },
      "source": [
        "## 1.2 Open-Ended Questions (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fAmNEvyO-Z-"
      },
      "source": [
        "## **Q1. Understanding Self-Attention in Transformers (5 points)**\n",
        "\n",
        "### **Scenario**  \n",
        "Imagine you are designing a **language model** for **text generation**, such as an **autocorrect system**. Your model needs to understand the relationships between words in a sentence, regardless of their positions. Traditional models like RNNs struggle with **long-range dependencies**, meaning that words appearing far apart in a sentence may not influence each other effectively.  \n",
        "\n",
        "### **Question**  \n",
        "How does the **self-attention mechanism** help capture long-range dependencies in a sequence, and why is it more effective than recurrent layers in this regard?  \n",
        "\n",
        "### **Hint**  \n",
        "Think about how **queries, keys, and values** are used in self-attention and how the **dot-product attention mechanism** allows the model to focus on **all tokens simultaneously** rather than sequentially.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKscwjJXLVCZ"
      },
      "source": [
        "\n",
        "### **Your Answer:**  Self-attention helps to capture long range dependencies in a sequence because it uses the dot-product attentuon mechanism, which allows the model to look at all tokens in the data rather than one by one, so the weights from all tokens can influence the model for the word the model is trying to predict. Its more efficient since reccurent layers operate sequentially, so tokens that are farther from the word of interest have less influence while tokens closer are more influential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FidxxiYHKY4b"
      },
      "source": [
        "## **Q2. The Role of Positional Embeddings in Transformers (5 points)**\n",
        "\n",
        "### **Scenario**  \n",
        "In models like **RNNs**, token order is naturally encoded by processing words sequentially. However, in **Transformers**, words are processed **in parallel**, which means the model lacks any **built-in notion of word order**. Without a mechanism to encode order, a Transformer would treat a sentence like \"The cat chased the dog\" the same as \"The dog chased the cat.\"  \n",
        "\n",
        "### **Question**  \n",
        "Why do Transformers require **positional embeddings**, and how do **sinusoidal positional encodings** help the model differentiate between token positions?  \n",
        "\n",
        "### **Hint**  \n",
        "Consider how positional embeddings are **added to word embeddings** and how sinusoidal functions provide a **continuous, fixed representation** of position information.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LebPjoqhLXjn"
      },
      "source": [
        "\n",
        "### **Your Answer:**   The positional embeddings retain information about where input data (words) are located in the sequence (sentence), which is important so the transformer has the spatial relationships of the words as well as the contextual ones. The sinusoidal positional encodings help to differentiate between token positions because they provide a smooth continuous method of locating position that is bound between -1 and 1, which is easier for the model to understand and provides global and local location relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwy0fVSQLAcI"
      },
      "source": [
        "## **Q3. Feature Extraction in Convolutional Neural Networks (CNNs) (5 points)**  \n",
        "\n",
        "### **Scenario**  \n",
        "You are developing a **facial recognition system** using images captured from security cameras. To correctly classify faces, your system must learn to recognize **features such as edges, textures, and patterns**, rather than memorizing specific pixel values. Fully connected layers struggle with this because they lack spatial awareness.  \n",
        "\n",
        "### **Question**  \n",
        "How does the **convolution operation** in CNNs enable the model to learn and extract important features from images? What advantages does convolution provide over fully connected layers for image-based tasks?  \n",
        "\n",
        "### **Hint**  \n",
        "Think about how **local receptive fields** and **parameter sharing** allow CNNs to capture **spatial hierarchies** while reducing the number of parameters compared to fully connected layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1z8K8uCLZ_s"
      },
      "source": [
        "\n",
        "### **Your Answer:**  The convolution operation is an effective way of extracting important features since it uses a filter to connect pixels in close proximity of each other which helps to extract useful features that are larger than the pixel scale . Convolution is advantagous because it reduces the amount of parameters that the model has to interpret when compared to fully connected layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxIQ2ar6LFvh"
      },
      "source": [
        "## **Q4. The Effect of Max-Pooling in CNNs (5 points)**  \n",
        "\n",
        "### **Scenario**  \n",
        "Suppose you are working on an **image classification task** where your model needs to recognize objects in pictures taken from **different angles and distances**. You notice that small shifts in an objectâ€™s position within the image can cause **inconsistent feature activations**, making the model **sensitive to slight changes**.  \n",
        "\n",
        "### **Question**  \n",
        "How does **max-pooling** help improve the **spatial invariance** of a CNN, and what effect does it have on the **size and complexity** of feature maps?  \n",
        "\n",
        "### **Hint**  \n",
        "Consider how **max-pooling reduces dimensionality**, improves **robustness to small translations**, and prevents the model from being **overly sensitive to exact pixel positions**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExGTfx98LcM-"
      },
      "source": [
        "\n",
        "### **Your Answer:**   Max-pooling improves the spatial invariance of a CNN since it decreases the size and complexity of the feature map by taking the maximum pixel value in an area as the representative value of that area. This decreases the amount of variance in a small area and ensures the model focuses on the most important parameters in since it generalizes a value (the max value) for all of the positions in the area of focus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXR0mrSQsvxg"
      },
      "source": [
        "# **Section 2: Implementing Transformers (45 Points)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL1MTuqYwU8p"
      },
      "source": [
        "## **Objective**\n",
        "In this section, you will implement a simplified Transformer block from scratch. Your implementation should focus on four core components:\n",
        "\n",
        "1. **Self-Attention:** Compute queries, keys, and values from the input, then apply the scaled dot-product attention mechanism.\n",
        "2. **Feed-Forward Neural Network (FNN):** Build a two-layer fully connected network with a ReLU activation to introduce non-linearity.\n",
        "3. **Positional Embeddings:** Compute sinusoidal positional encodings and add them to the input embeddings to provide sequence order information.\n",
        "4. **Transformer Block Assembly:** Integrate the components using residual connections to form a complete Transformer block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "dgUQ5RlHwhtF"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5oqqiiAxzSC"
      },
      "source": [
        "## Task 2.1: Self-Attention (Missing Blocks: 2 / Total Points: 10)\n",
        "\n",
        "### Description\n",
        "Implement the self-attention mechanism. Use the provided fixed weight matrices (do not modify these initializations) to compute the query (Q), key (K), and value (V) vectors from the input tensor. Then compute the scaled dot-product attention by:\n",
        "- Calculating the dot product between Q and the transpose of K, scaled by the square root of the model dimension.\n",
        "- Applying a softmax (with numerical stability adjustments) to obtain the attention weights.\n",
        "- Multiplying these weights with V to produce the self-attention output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "pzcIYMaEyVKf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 2.1 - Self-Attention Output:\n",
            "[[[1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157\n",
            "   1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157]\n",
            "  [1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157\n",
            "   1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157]\n",
            "  [1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157\n",
            "   1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157]\n",
            "  [1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157\n",
            "   1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157]\n",
            "  [1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157\n",
            "   1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157]]\n",
            "\n",
            " [[1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157\n",
            "   1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157]\n",
            "  [1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157\n",
            "   1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157]\n",
            "  [1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157\n",
            "   1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157]\n",
            "  [1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157\n",
            "   1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157]\n",
            "  [1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157\n",
            "   1.40868461e-157 1.40868461e-157 1.40868461e-157 1.40868461e-157]]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2, 5, 8)"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def self_attention(x):\n",
        "    \"\"\"\n",
        "    Compute self-attention on the input tensor.\n",
        "\n",
        "    Args:\n",
        "      x: Input tensor of shape (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "    Returns:\n",
        "      attn_output: Tensor after applying self-attention.\n",
        "    \"\"\"\n",
        "    d_model = x.shape[-1]\n",
        "\n",
        "    W_Q = np.full((d_model, d_model), 1.0)   # All ones\n",
        "    W_K = np.full((d_model, d_model), 2.0)     # All twos\n",
        "    W_V = np.full((d_model, d_model), 3.0)     # All threes\n",
        "\n",
        "    # #############################################################\n",
        "    # Your Turn: Compute queries, keys, and values from input x.\n",
        "    # -------------------------------------------------------------\n",
        "    q = x @ W_Q\n",
        "    k = x @ W_K\n",
        "    v = x @ W_V\n",
        "    # #############################################################\n",
        "\n",
        "\n",
        "    # #############################################################\n",
        "    # Your Turn: Compute the scaled dot-product attention.\n",
        "    #   - Compute scores = Q * K^T / sqrt(d_model)\n",
        "    #   - Apply softmax to scores (subtracting max for numerical stability)\n",
        "    #   - Compute attn_output = weights * V\n",
        "    # -------------------------------------------------------------\n",
        "    scores = q @ k.transpose(0,2,1)/np.sqrt(d_model)\n",
        "    weights = (np.exp(scores - np.max(scores)))/(np.sum(np.exp(scores))) \n",
        "    attn_output = weights @ v\n",
        "    # #############################################################\n",
        "\n",
        "    return attn_output\n",
        "  \n",
        "\n",
        "dummy_input_sa = np.ones((2, 5, 8))\n",
        "sa_output = self_attention(dummy_input_sa)\n",
        "print(\"Task 2.1 - Self-Attention Output:\")\n",
        "print(sa_output)\n",
        "sa_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIPsXNFazMwd"
      },
      "source": [
        "## Task 2.2: Feed-Forward Network (Missing Blocks: 3 / Total Points: 15)\n",
        "\n",
        "### Description\n",
        "Implement a two-layer feed-forward network that processes the output from the self-attention sub-layer. Use the fixed weight and bias initializations provided:\n",
        "- **First layer:** Expands the dimension by a factor of 4 (weights set to all ones, biases set to zeros).\n",
        "- **Second layer:** Projects back to the original dimension (weights set to all ones, biases set to zeros).\n",
        "\n",
        "Apply a ReLU activation after the first layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "MMtVQNmWzMbM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Task 2.2 - Feed-Forward Network Output:\n",
            "[[[6144. 6144. 6144. 6144. 6144. 6144. 6144. 6144.]\n",
            "  [6144. 6144. 6144. 6144. 6144. 6144. 6144. 6144.]\n",
            "  [6144. 6144. 6144. 6144. 6144. 6144. 6144. 6144.]\n",
            "  [6144. 6144. 6144. 6144. 6144. 6144. 6144. 6144.]\n",
            "  [6144. 6144. 6144. 6144. 6144. 6144. 6144. 6144.]]\n",
            "\n",
            " [[6144. 6144. 6144. 6144. 6144. 6144. 6144. 6144.]\n",
            "  [6144. 6144. 6144. 6144. 6144. 6144. 6144. 6144.]\n",
            "  [6144. 6144. 6144. 6144. 6144. 6144. 6144. 6144.]\n",
            "  [6144. 6144. 6144. 6144. 6144. 6144. 6144. 6144.]\n",
            "  [6144. 6144. 6144. 6144. 6144. 6144. 6144. 6144.]]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2, 5, 8)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def feed_forward(x):\n",
        "    \"\"\"\n",
        "    Compute the output of a two-layer feed-forward network.\n",
        "\n",
        "    Args:\n",
        "      x: Input tensor of shape (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "    Returns:\n",
        "      output: Tensor after applying the feed-forward network.\n",
        "    \"\"\"\n",
        "    d_model = x.shape[-1]\n",
        "    d_ff = d_model * 4  # Expansion factor\n",
        "\n",
        "    #############################################################\n",
        "    # Your Turn: Define weights and biases for the first linear layer.\n",
        "    # -----------------------------------------------------------\n",
        "    w1 = np.ones((d_model, d_ff))\n",
        "    biases1 = np.zeros((d_ff))\n",
        "    \n",
        "    #############################################################\n",
        "\n",
        "    # #############################################################\n",
        "    # Your Turn: Define weights and biases for the second linear layer.\n",
        "    # -------------------------------------------------------------\n",
        "    w2 = np.ones((d_ff, d_model))\n",
        "    biases2 = np.zeros((d_model))\n",
        "    # #############################################################\n",
        "\n",
        "    # #############################################################\n",
        "    # Your Turn: Compute the feed-forward output.\n",
        "    #   - Compute hidden = x * W1 + b1, then apply ReLU.\n",
        "    #   - Compute output = hidden * W2 + b2.\n",
        "    # -------------------------------------------------------------\n",
        "    hidden = x @ w1 + biases1\n",
        "    relu_hidden = np.maximum(0,hidden)\n",
        "    output = relu_hidden @ w2 + biases2\n",
        "    # #############################################################\n",
        "\n",
        "    return output\n",
        "\n",
        "dummy_input_ffn = np.full((2, 5, 8), 24.0)\n",
        "ffn_output = feed_forward(dummy_input_ffn)\n",
        "print(\"\\nTask 2.2 - Feed-Forward Network Output:\")\n",
        "print(ffn_output)\n",
        "ffn_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI8SpUgIzw4_"
      },
      "source": [
        "## Task 2.3: Positional Embeddings (Missing Block: 1 / Total Points: 10)\n",
        "\n",
        "### Description\n",
        "Compute sinusoidal positional encodings and add them to the input tensor. For each position `pos` and each dimension `i`, you will:\n",
        "- Compute `angle = pos / (10000^(2*(i//2)/d_model))`.\n",
        "- Use `sin(angle)` for even indices and `cos(angle)` for odd indices.\n",
        "Finally, add the computed positional encodings to the input tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "1nSifGs2z4RJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Task 2.3 - Positional Encoding Output (first example):\n",
            "[[1.         2.         1.         2.         1.         2.\n",
            "  1.         2.        ]\n",
            " [1.84147098 1.54030231 1.09983342 1.99500417 1.00999983 1.99995\n",
            "  1.001      1.9999995 ]\n",
            " [1.90929743 0.58385316 1.19866933 1.98006658 1.01999867 1.99980001\n",
            "  1.002      1.999998  ]\n",
            " [1.14112001 0.0100075  1.29552021 1.95533649 1.0299955  1.99955003\n",
            "  1.003      1.9999955 ]\n",
            " [0.2431975  0.34635638 1.38941834 1.92106099 1.03998933 1.99920011\n",
            "  1.00399999 1.999992  ]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(5, 8)"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def positional_encoding(x):\n",
        "    \"\"\"\n",
        "    Add sinusoidal positional encodings to the input tensor.\n",
        "\n",
        "    Args:\n",
        "      x: Input tensor of shape (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "    Returns:\n",
        "      Tensor with positional encodings added.\n",
        "    \"\"\"\n",
        "    batch_size, seq_length, d_model = x.shape\n",
        "    pos_enc = np.zeros((seq_length, d_model))\n",
        "\n",
        "    # #############################################################\n",
        "    # Your Turn: Compute sinusoidal positional encodings.\n",
        "    #   For each position pos and each dimension i:\n",
        "    #     angle = pos / (10000^(2*(i//2)/d_model))\n",
        "    #     Use sin(angle) if i is even, cos(angle) if i is odd.\n",
        "    # -------------------------------------------------------------\n",
        "    #\n",
        "    for pos in range(seq_length):\n",
        "        for i in range(d_model):\n",
        "            angle = pos / (10000 ** (2 * (i // 2) / d_model))\n",
        "            if i % 2 == 0:\n",
        "                pos_enc[pos, i] = np.sin(angle)\n",
        "            else:\n",
        "                pos_enc[pos, i] = np.cos(angle)\n",
        "    # #############################################################\n",
        "    return x + pos_enc\n",
        "\n",
        "dummy_input_pe = np.ones((2, 5, 8))\n",
        "pe_output = positional_encoding(dummy_input_pe)\n",
        "print(\"\\nTask 2.3 - Positional Encoding Output (first example):\")\n",
        "print(pe_output[0])\n",
        "pe_output[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmaZ9Bfj0F2w"
      },
      "source": [
        "## Task 2.4: Transformer Block Assembly (Missing Blocks: 3 / Total Points: 10)\n",
        "\n",
        "### Description\n",
        "Assemble the full Transformer block by integrating the positional encoding, self-attention, and feed-forward network using residual connections. Your tasks are:\n",
        "- Apply the positional encoding to the input tensor.\n",
        "- Compute the self-attention output and add it to the input (residual connection).\n",
        "- Compute the feed-forward network output and add it to the result (residual connection)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "IXlxiw0X0Lvg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Task 2.4 - Transformer Block Output:\n",
            "[[[385.         386.         385.         386.         385.\n",
            "   386.         385.         386.        ]\n",
            "  [401.44339758 401.1422289  400.70176001 401.59693076 400.61192643\n",
            "   401.6018766  400.6029266  401.6019261 ]\n",
            "  [376.1071589  374.78171463 375.3965308  376.17792805 375.21786014\n",
            "   376.19766148 375.19986147 376.19785947]\n",
            "  [335.04592758 333.91481508 335.20032778 335.86014406 334.93480307\n",
            "   335.90435761 334.90780757 335.90480307]\n",
            "  [318.42606632 318.52922519 319.57228715 320.1039298  319.22285814\n",
            "   320.18206892 319.1868688  320.18286081]]\n",
            "\n",
            " [[385.         386.         385.         386.         385.\n",
            "   386.         385.         386.        ]\n",
            "  [401.44339758 401.1422289  400.70176001 401.59693076 400.61192643\n",
            "   401.6018766  400.6029266  401.6019261 ]\n",
            "  [376.1071589  374.78171463 375.3965308  376.17792805 375.21786014\n",
            "   376.19766148 375.19986147 376.19785947]\n",
            "  [335.04592758 333.91481508 335.20032778 335.86014406 334.93480307\n",
            "   335.90435761 334.90780757 335.90480307]\n",
            "  [318.42606632 318.52922519 319.57228715 320.1039298  319.22285814\n",
            "   320.18206892 319.1868688  320.18286081]]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/6n/cwl6lb1j5j54r8zxq47cqpbr0000gn/T/ipykernel_32585/1288163090.py:33: RuntimeWarning: overflow encountered in exp\n",
            "  weights = (np.exp(scores - np.max(scores)))/(np.sum(np.exp(scores)))\n"
          ]
        }
      ],
      "source": [
        "def transformer_block(x):\n",
        "    \"\"\"\n",
        "    Assemble the Transformer block.\n",
        "\n",
        "    Args:\n",
        "      x: Input tensor of shape (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "    Returns:\n",
        "      Output tensor after processing through the Transformer block.\n",
        "    \"\"\"\n",
        "    # #############################################################\n",
        "    # Your Turn: Apply positional encoding.\n",
        "    # -------------------------------------------------------------\n",
        "    encode = positional_encoding(x)\n",
        "    # #############################################################\n",
        "\n",
        "    # #############################################################\n",
        "    # Your Turn: Compute self-attention output and add a residual connection.\n",
        "    # -------------------------------------------------------------\n",
        "    self_atten = self_attention(encode) + encode\n",
        "    # #############################################################\n",
        "\n",
        "    # #############################################################\n",
        "    # Your Turn: Compute feed-forward network output and add a residual connection.\n",
        "    # -------------------------------------------------------------\n",
        "    x = feed_forward(self_atten) + self_atten\n",
        "    # #############################################################\n",
        "\n",
        "    return x\n",
        "\n",
        "dummy_input_tb = np.ones((2, 5, 8))  # Fixed dummy input: tensor of ones\n",
        "tb_output = transformer_block(dummy_input_tb)\n",
        "print(\"\\nTask 2.4 - Transformer Block Output:\")\n",
        "print(tb_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aNqYAzu16CV"
      },
      "source": [
        "# Section 3: Implementing CNNs (15 Points)\n",
        "\n",
        "## Objective\n",
        "In this section you will implement a simplified Convolutional Neural Network (CNN). Your implementation focuses on two core components:\n",
        "1. **Convolution:** Apply a convolution operation on a 2D input (image) using a fixed kernel. The convolution is performed by sliding the kernel over the image, computing element-wise products, and summing them to produce a feature map.\n",
        "2. **Max-Pooling:** Apply a max-pooling operation on the feature map to downsample it by selecting the maximum value from non-overlapping pooling regions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg4xM--z2LIL"
      },
      "source": [
        "## Task 3.1: Convolution (Missing Blocks: 1 / Total Points: 10)\n",
        "\n",
        "### Description\n",
        "Implement the convolution operation for a 2D input image using a fixed kernel. The convolution is performed by sliding the kernel over the image, computing the element-wise product between the kernel and the current patch, and summing the results to form the output pixel value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "o_2Gdtuc2O9C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 3.1 - Convolution Output:\n",
            "[[-5. -5. -5.]\n",
            " [-5. -5. -5.]\n",
            " [-5. -5. -5.]]\n"
          ]
        }
      ],
      "source": [
        "def conv_layer(x, kernel):\n",
        "    \"\"\"\n",
        "    Apply convolution on the input image using the given kernel.\n",
        "\n",
        "    Args:\n",
        "      x: 2D input array (e.g., an image) of shape (H, W)\n",
        "      kernel: Convolution kernel (filter) of shape (kH, kW)\n",
        "\n",
        "    Returns:\n",
        "      Output array of shape (H - kH + 1, W - kW + 1)\n",
        "    \"\"\"\n",
        "    H, W = x.shape\n",
        "    kH, kW = kernel.shape\n",
        "    output_H = H - kH + 1\n",
        "    output_W = W - kW + 1\n",
        "    output = np.zeros((output_H, output_W))\n",
        "\n",
        "    # #############################################################\n",
        "    # Your Turn: Slide the kernel over the image and compute the dot product for each patch.\n",
        "    # -------------------------------------------------------------\n",
        "    for i in range(output_H):\n",
        "        for j in range(output_W):\n",
        "            patch = x[i:i+kH, j:j+kW]\n",
        "            output[i, j] = np.sum(patch * kernel)  # Output array of shape (H - kH + 1, W - kW + 1)\n",
        "    # #############################################################\n",
        "\n",
        "    return output\n",
        "\n",
        "dummy_image = np.array([[1, 2, 3, 4],\n",
        "                        [5, 6, 7, 8],\n",
        "                        [9,10,11,12],\n",
        "                        [13,14,15,16]])\n",
        "kernel = np.array([[1, 0],\n",
        "                   [0, -1]])\n",
        "conv_output = conv_layer(dummy_image, kernel)\n",
        "print(\"Task 3.1 - Convolution Output:\")\n",
        "print(conv_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAMnmJxv2hkb"
      },
      "source": [
        "## Task 3.2: Max-Pooling (Missing Block: 1 / Total Points: 5)\n",
        "\n",
        "### Description\n",
        "Implement the max-pooling operation on a 2D feature map. The max-pooling operation divides the input into non-overlapping regions of a specified window size and selects the maximum value from each region, producing a downsampled output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "uwKhg4lK2krk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Task 3.2 - Max-Pooling Output:\n",
            "[[ 6.  4.]\n",
            " [12. 14.]]\n"
          ]
        }
      ],
      "source": [
        "def max_pooling(x, pool_size):\n",
        "    \"\"\"\n",
        "    Apply max-pooling to the input feature map.\n",
        "\n",
        "    Args:\n",
        "      x: 2D input array (e.g., a feature map)\n",
        "      pool_size: Size of the pooling window (assume square)\n",
        "\n",
        "    Returns:\n",
        "      Pooled output array.\n",
        "    \"\"\"\n",
        "    H, W = x.shape\n",
        "    new_H = H // pool_size\n",
        "    new_W = W // pool_size\n",
        "    pooled = np.zeros((new_H, new_W))\n",
        "\n",
        "    # #############################################################\n",
        "    # Your Turn: Divide the input into non-overlapping windows and compute\n",
        "    # the maximum value from each window.\n",
        "    # -------------------------------------------------------------\n",
        "    for i in range(new_H):\n",
        "      for j in range(new_W):\n",
        "          patch = x[i*pool_size:(i+1)*pool_size, j*pool_size:(j+1)*pool_size]\n",
        "          pooled[i, j] = np.max(patch)\n",
        "    # #############################################################\n",
        "\n",
        "    return pooled\n",
        "\n",
        "dummy_feature_map = np.array([[1, 3, 2, 4],\n",
        "                              [5, 6, 1, 2],\n",
        "                              [7, 8, 9, 10],\n",
        "                              [11,12,13,14]])\n",
        "pool_size = 2\n",
        "pool_output = max_pooling(dummy_feature_map, pool_size)\n",
        "print(\"\\nTask 3.2 - Max-Pooling Output:\")\n",
        "print(pool_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9UPmezPrWBL"
      },
      "source": [
        "# Declaration of Independent Work  \n",
        "\n",
        "I hereby declare that this assignment is entirely my own work and that I have neither given nor received unauthorized assistance in completing it. I have adhered to all the guidelines provided for this assignment and have cited all sources from which I derived data, ideas, or words, whether quoted directly or paraphrased.\n",
        "\n",
        "Furthermore, I understand that providing false declaration is a violation of the University of Arizona's honor code and will result in appropriate disciplinary action consistent with the severity of the violation.\n",
        "\n",
        "**Name:** Christian Ortmann  \n",
        "**Date:** 03/07/2025"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
