{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RMRL3uydTlk"
      },
      "source": [
        "<h1 align=\"center\">INFO621 - Advanced Machine Learning Applications</h1>\n",
        "\n",
        "<h2 align=\"center\"><strong>Homework 2: Neural Language and Sequence Modeling</strong></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkQOtjDHdTln"
      },
      "source": [
        "# Guidelines\n",
        "\n",
        "**Worth:** 10% of your final grade (**100 points total**)  \n",
        "**Submission Deadline:** Friday, February 21, 11:59 PM (Tucson Time)  \n",
        "\n",
        "\n",
        "### **Instructions**\n",
        "\n",
        "- For exercises involving code, write your solutions in the provided code chunks. You may add additional code chunks if needed.  \n",
        "- For exercises involving plots, ensure all axes and legends are labeled and give each plot an informative title.  \n",
        "- For exercises requiring descriptions or interpretations, use full sentences and provide clear, concise explanations.  \n",
        "\n",
        "### **Policies**\n",
        "\n",
        "**Sharing/Reusing Code Policy:**  \n",
        "You are allowed to use online resources (e.g., RStudio Community, StackOverflow) but **must explicitly cite** any external code you use or adapt. Failure to do so will be considered plagiarism, regardless of the source.  \n",
        "\n",
        "**Late Submission Policy:**  \n",
        "- **Less than 1 day late:** -25% of available points.  \n",
        "- **1-7 days late:** -50% of available points.  \n",
        "- **7 days or more late:** No credit will be awarded, and feedback will not be provided.  \n",
        "\n",
        "**Declaration of Independent Work:**  \n",
        "You must acknowledge your submission as your independent work by including your **name** and **date** at the end of the \"Declaration of Independent Work\" section.  \n",
        "\n",
        "### **Grading**\n",
        "\n",
        "- **Total Points:** 100 points.\n",
        "\n",
        "- **Grade Breakdown:**  \n",
        "  - **Part 1 (40 Points Total):**  \n",
        "    - Multiple-Choice Questions: 5 questions, 4 points each.  \n",
        "    - Descriptive Questions: 4 questions, 5 points each.  \n",
        "\n",
        "  - **Part 2 (40 Points Total):**  \n",
        "    - 10 code completion tasks, 4 points each.  \n",
        "\n",
        "  - **Part 3 (20 Points Total):**  \n",
        "    - 4 code completion tasks, 5 points each.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zIcc3JNeeOC"
      },
      "source": [
        "# Part 1. Written Questions (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yupqEgrdTlo"
      },
      "source": [
        "- **Multiple-Choice Questions**: 5 questions, 4 points each  \n",
        "- **Descriptive Questions**: 4 questions, 5 points each"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMPMDe3HdTlp"
      },
      "source": [
        "## 1.1 Multiple-Choice Questions (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fRUVMbVdTlp"
      },
      "source": [
        "1. **What is the key role of the activation function in a perceptron? (5 points)**\n",
        "\n",
        "   - A) To initialize weights  \n",
        "   - B) To compute the weighted sum of inputs  \n",
        "   - C) To introduce non-linearity into the model\n",
        "   - D) To apply regularization  \n",
        "   - E) To update weights through gradient descent  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcsi6CeYdTlp"
      },
      "source": [
        "**Your Answer**: C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OpdpgIWdTlq"
      },
      "source": [
        "2. **Which of the following is NOT a type of neural network activation function? (5 points)**\n",
        "\n",
        "   - A) Sigmoid  \n",
        "   - B) Tanh  \n",
        "   - C) ReLU  \n",
        "   - D) Softmax  \n",
        "   - E) Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDQVb3OsdTlq"
      },
      "source": [
        "**Your Answer**: E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY6lnPLadTlq"
      },
      "source": [
        "3. **What is backpropagation primarily used for in a neural network? (5 points)**\n",
        "\n",
        "   - A) Data normalization  \n",
        "   - B) Computing gradients for weight updates\n",
        "   - C) Generating predictions from the model  \n",
        "   - D) Preventing overfitting  \n",
        "   - E) Reducing the training time  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcKV6wP_dTlr"
      },
      "source": [
        "**Your Answer**: B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vztxrqpgdTlr"
      },
      "source": [
        "4. **Which regularization method involves halting training at a point to avoid overfitting? (5 points)**\n",
        "\n",
        "   - A) Dropout  \n",
        "   - B) Batch Normalization  \n",
        "   - C) Weight Decay  \n",
        "   - D) Early Stopping\n",
        "   - E) Adaptive Learning  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apvsq-yWdTlr"
      },
      "source": [
        "**Your Answer**: D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNpCLl_udTlr"
      },
      "source": [
        "5. **What is one major advantage of using mini-batch gradient descent over full-batch gradient descent? (5 points)**\n",
        "\n",
        "   - A) It avoids the vanishing gradient problem\n",
        "   - B) It improves the generalization of the model\n",
        "   - C) It balances computational efficiency and convergence stability\n",
        "   - D) It guarantees convergence to a global minimum  \n",
        "   - E) It completely eliminates overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47Tja0x4dTlr"
      },
      "source": [
        "**Your Answer**: C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zclpWQNodTlr"
      },
      "source": [
        "## 1.2 Open-Ended Questions (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5JkM2YwdTlr"
      },
      "source": [
        "### Q1. Perceptron and XOR\n",
        "\n",
        "**Scenario**:\n",
        "Imagine you are tackling a binary classification task where the data exhibits a pattern similar to the XOR problem—a classic example where data points cannot be separated by a single straight line. This scenario illustrates the challenge of linear separability and sets the stage for understanding why a basic model might struggle with such tasks.\n",
        "\n",
        "**Question**:\n",
        "Why is a single-layer perceptron unable to solve the XOR problem, and how does adding multiple layers to a neural network overcome this limitation?\n",
        "\n",
        "**Hint**:\n",
        "Consider the concept of linear separability and how deeper networks can perform non-linear transformations.\n",
        "\n",
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPJOdMEMdTls"
      },
      "source": [
        "Data that is not linearly separable, which is when a data set has two classes of data points that can be separated by a straight line in space, must utilize multiple layers with hidden layers between them, which are able to approximate non-linear relationships because they contain activation function which can estimate non-linear relationships, and when stacked with multiple layers, the non-linear relationship of the two classes in the XOR problem can be estimated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmUGc4vmdTls"
      },
      "source": [
        "### Q2. Loss Functions: Binary Cross-Entropy vs. Mean Squared Error\n",
        "\n",
        "**Scenario**\n",
        "Imagine you are training a neural network to distinguish between two classes, such as identifying whether an email is spam or not. You need a reliable way to quantify how well your model's predictions match the actual outcomes. This scenario highlights the importance of selecting the right loss function based on the task at hand.  \n",
        "\n",
        "**Question**\n",
        "\n",
        "What is the purpose of the loss function in a neural network?**\n",
        "**How do binary cross-entropy (BCE) and mean squared error (MSE) differ in their application?\n",
        "\n",
        "**Hint**  \n",
        "Think about the types of tasks (**classification vs. regression**) each loss function is best suited for.\n",
        "\n",
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m5Mkx_FdTls"
      },
      "source": [
        "The loss function is used to measure the cost of incorrect predictions. In classification, BCE is used to measure loss as it estimates how far off the probability prediction is from the actual value, whereas MSE measures the average of the squared difference between the predicted value and the actual value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PqFfdcfdTls"
      },
      "source": [
        "### Q3. Importance of Non-Linearity  \n",
        "\n",
        "**Scenario**\n",
        "\n",
        "Consider a scenario where you are building a **neural network** for a **complex task**, such as **recognizing handwritten digits**. The ability to distinguish between similar but distinct patterns hinges on the network’s capacity to model **non-linear relationships**. Without **non-linear transformations**, the network's power would be drastically limited.  \n",
        "\n",
        "**Question**  \n",
        "Why is non-linearity critical in neural networks, and what would be the consequence if the network's activation functions were purely linear?  \n",
        "\n",
        "**Hint**  \n",
        "Think about how **stacking linear layers** without **non-linear transformations** would affect the **model’s expressive power**.  \n",
        "\n",
        "**Answer**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEu81T8ddTls"
      },
      "source": [
        "Non-linearity is critical because it ensures that the neural network can understand problems that are not linearly separable, which in the real world is a majority of problems. Because handwriting varies so much, non-linearity is critical to ensure that the model can choose all the possible presentations of each value to ensure that the variation in handwriting is accounted for. If the activation function is linear, then the model would only be able to approximate the 26 letters of one person's handwriting and would encounter larger error's when classifying new handwriting because it cannot account for the new variation in letter formation from the new handwriting. Essentially it would be akin to using one linear function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6t8ArA-dTls"
      },
      "source": [
        "### Q4. Dropout as a Regularization Method\n",
        "\n",
        "**Scenario**  \n",
        "Imagine training a **deep neural network** that performs **exceptionally well on the training data** but **fails to generalize to unseen data**—a common problem known as **overfitting**. To combat this, you decide to incorporate a technique that forces the network to learn **robust and redundant features**, thereby improving its **generalization ability**.  \n",
        "\n",
        "**Question**  \n",
        "What is dropout, and why is it particularly effective as a regularization method in deep neural networks?\n",
        "\n",
        "**Hint**  \n",
        "Consider how **dropout temporarily disables neurons during training** and what effect this has on the **network’s learning process**.  \n",
        "\n",
        "**Answer**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hxph1J2dTls"
      },
      "source": [
        "Dropout is a regularization process that randomly drops some of the activations to 0 during training. For each layer ~50% are randomly dropped to 0 and is effective because it forces the network to not rely on one specific node, improving generalization abilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIpX3ue3eY0m"
      },
      "source": [
        "# Part 2. Implementing a Simple Neural Network from Scratch (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymS1CJ51e9F2"
      },
      "source": [
        "## **Objective**\n",
        "In this homework, you will implement various components of a simple neural network from scratch. You are required to complete the following tasks:\n",
        "\n",
        "1. **Perceptron Basics**: Implement the forward pass of a perceptron, including computing the weighted sum and applying the sigmoid activation function.\n",
        "2. **Multi-Layer Perceptron**: Extend your implementation to include a two-layer neural network, using ReLU for the hidden layer and softmax for the output layer.\n",
        "3. **Loss Function**: Implement the binary cross-entropy loss function to evaluate the model's performance.\n",
        "4. **Backpropagation**: Compute the gradients of the loss with respect to the weights and biases using the backpropagation algorithm.\n",
        "5. **Training the Neural Network**: Train the neural network using gradient descent by iteratively updating the weights and biases to minimize the loss.\n",
        "\n",
        "By completing these tasks, you will gain hands-on experience in building and training neural networks and understanding their underlying mechanics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "nroIjqHZesao"
      },
      "outputs": [],
      "source": [
        "# Necessary Imports\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmGNqqSBXKz1"
      },
      "source": [
        "## **2.1 Perceptron Basics (2 missing blocks, 8 points in total)**\n",
        "\n",
        "A perceptron calculates a weighted sum of inputs and applies an activation function to produce an output. The sigmoid activation function is defined as:\n",
        "\n",
        "$\\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}$\n",
        "\n",
        "### **Task**:\n",
        "1. Compute the weighted sum $z = x \\cdot w + b$, where $x$ is the input vector, $w$ is the weight vector, and $b$ is the bias term.\n",
        "2. Apply the sigmoid activation function to $z$ to compute the perceptron output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "8pS0Br4yXQ7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: 0.8698915256370021\n"
          ]
        }
      ],
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, weights, bias):\n",
        "        \"\"\"\n",
        "        Initialize the Perceptron.\n",
        "\n",
        "        Parameters:\n",
        "        - weights: Array of weights for the input features.\n",
        "        - bias: Bias term.\n",
        "        \"\"\"\n",
        "        self.weights = np.array(weights)\n",
        "        self.bias = bias\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Sigmoid activation function.\n",
        "\n",
        "        Parameters:\n",
        "        - z: Weighted sum of inputs.\n",
        "\n",
        "        Returns:\n",
        "        - Sigmoid activation value.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        return 1/((1+np.exp(-z)))\n",
        "    \n",
        "\n",
        "        #############################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the perceptron.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input features.\n",
        "\n",
        "        Returns:\n",
        "        - Output after applying sigmoid activation.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        z = (np.dot(x,self.weights) + self.bias)\n",
        "        \n",
        "        return self.sigmoid(z)\n",
        "    \n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "\n",
        "# Example usage\n",
        "weights = [0.1, 0.2, 0.3]\n",
        "bias = 0.5\n",
        "perceptron = Perceptron(weights, bias)\n",
        "x = np.array([1.0, 2.0, 3.0])\n",
        "output = perceptron.forward(x)\n",
        "print(\"Output:\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYlL5cHXXawP"
      },
      "source": [
        "## **2.2 Multi-Layer Perceptron (3 missing blocks, 12 points in total)**\n",
        "\n",
        "A multi-layer perceptron (MLP) consists of layers of neurons, each performing a weighted sum followed by a non-linear activation function.\n",
        "\n",
        "The **ReLU** (Rectified Linear Unit) activation function is defined as:\n",
        "\n",
        "$\\text{ReLU}(z) = \\max(0, z)$\n",
        "\n",
        "The **softmax** function for the output layer is defined as:\n",
        "\n",
        "$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$\n",
        "\n",
        "### **Task**:\n",
        "1. Compute the hidden layer activations: $ z_1 = X \\cdot W_1 + b_1 $, $ a_1 = \\text{ReLU}(z_1) $.\n",
        "2. Compute the output layer activations: $ z_2 = a_1 \\cdot W_2 + b_2 $, $ a_2 = \\text{softmax}(z_2) $.\n",
        "3. Write the forward function of the Perceptron.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "43vO-0zgXiY7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output Activations: [[0.03535013 0.14742539]\n",
            " [0.04463957 0.21925631]\n",
            " [0.03730286 0.16661751]\n",
            " [0.03752844 0.13297376]\n",
            " [0.03545425 0.14345179]]\n"
          ]
        }
      ],
      "source": [
        "class MultiLayerPerceptron:\n",
        "    def __init__(self, W1, b1, W2, b2):\n",
        "        \"\"\"\n",
        "        Initialize the Multi-Layer Perceptron.\n",
        "\n",
        "        Parameters:\n",
        "        - W1: Weights for the hidden layer.\n",
        "        - b1: Biases for the hidden layer.\n",
        "        - W2: Weights for the output layer.\n",
        "        - b2: Biases for the output layer.\n",
        "        \"\"\"\n",
        "        self.W1 = np.array(W1)\n",
        "        self.b1 = np.array(b1)\n",
        "        self.W2 = np.array(W2)\n",
        "        self.b2 = np.array(b2)\n",
        "\n",
        "    def relu(self, z):\n",
        "        \"\"\"\n",
        "        ReLU activation function.\n",
        "\n",
        "        Parameters:\n",
        "        - z: Weighted sum of inputs.\n",
        "\n",
        "        Returns:\n",
        "        - ReLU activation value.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        return np.maximum(0,z)\n",
        "\n",
        "        # \n",
        "        #############################################################\n",
        "\n",
        "    def softmax(self, z):\n",
        "        \"\"\"\n",
        "        Softmax activation function.\n",
        "\n",
        "        Parameters:\n",
        "        - z: Weighted sum of inputs.\n",
        "\n",
        "        Returns:\n",
        "        - Softmax probabilities.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        return (np.exp(z))/(np.sum(np.exp(z)))\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass for the multi-layer perceptron.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features.\n",
        "\n",
        "        Returns:\n",
        "        - Output probabilities.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        \n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        z1 = np.dot(X,self.W1)+self.b1\n",
        "        a1 = self.relu(z1)\n",
        "        z2 = np.dot(a1,self.W2)+self.b2\n",
        "        a2 = self.softmax(z2)\n",
        "        \n",
        "        return a2\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "\n",
        "# Example usage\n",
        "W1 = np.random.rand(3, 4)\n",
        "b1 = np.random.rand(4)\n",
        "W2 = np.random.rand(4, 2)\n",
        "b2 = np.random.rand(2)\n",
        "X = np.random.rand(5, 3)\n",
        "mlp = MultiLayerPerceptron(W1, b1, W2, b2)\n",
        "output = mlp.forward(X)\n",
        "print(\"Output Activations:\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx8YJb4uZ8z6"
      },
      "source": [
        "## **2.3 Loss Function (1 missing blocks, 4 points in total)**\n",
        "\n",
        "The binary cross-entropy loss quantifies the error between predicted probabilities ($y_{pred}$) and actual labels ($y_{true}$). The formula is:\n",
        "\n",
        "$L = -\\left(y_{true} \\cdot \\log(y_{pred}) + (1 - y_{true}) \\cdot \\log(1 - y_{pred})\\right)$\n",
        "\n",
        "### **Task**:\n",
        "1. Compute the loss for each sample using the formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1Vf7zN2Z8TS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.47703580218215375\n"
          ]
        }
      ],
      "source": [
        "class BinaryCrossEntropy:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the BinaryCrossEntropy class.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def compute(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute binary cross-entropy loss.\n",
        "\n",
        "        Parameters:\n",
        "        - y_true: Ground truth labels.\n",
        "        - y_pred: Predicted probabilities.\n",
        "\n",
        "        Returns:\n",
        "        - Binary cross-entropy loss.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        return np.mean(-(y_true * np.log(y_pred)) + ((1-y_true)*(1-y_pred)))\n",
        "\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "\n",
        "# Example usage\n",
        "y_true = np.array([1, 0, 1, 1, 0])\n",
        "y_pred = np.array([0.9, 0.2, 0.8, 0.7, 0.1])\n",
        "bce = BinaryCrossEntropy()\n",
        "loss = bce.compute(y_true, y_pred)\n",
        "print(\"Loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzDVIq98Xjv4"
      },
      "source": [
        "## **2.4 Backpropagation (1 missing blocks, 4 points in total)**\n",
        "\n",
        "In this task, you will compute:\n",
        "1. The gradient of the loss with respect to the outputs ($ \\frac{\\partial L}{\\partial \\text{output}} $).\n",
        "2. The gradients of the loss with respect to the weights ($ \\frac{\\partial L}{\\partial W} $) and biases ($ \\frac{\\partial L}{\\partial b} $) using the chain rule.\n",
        "\n",
        "### **Task**:\n",
        "Complete the `backpropagation` function by:\n",
        "1. Computing the gradient of the loss w.r.t. outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifNvc27WakFw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient of Loss w.r.t Weights: [[1.6]\n",
            " [2. ]]\n",
            "Gradient of Loss w.r.t Biases: 0.4\n"
          ]
        }
      ],
      "source": [
        "class Backpropagation:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the Backpropagation class.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def compute(self, X, y_true, weights, biases, activations):\n",
        "        \"\"\"\n",
        "        Compute gradients for weights and biases.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data.\n",
        "        - y_true: Ground truth labels.\n",
        "        - weights: Weight matrix.\n",
        "        - biases: Bias vector.\n",
        "        - activations: Predicted outputs.\n",
        "\n",
        "        Returns:\n",
        "        - grad_weights: Gradient of loss w.r.t weights.\n",
        "        - grad_biases: Gradient of loss w.r.t biases.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        d = activations - y_true\n",
        "        grad_weights = (X.T @ d)  \n",
        "        grad_biases = np.sum(d) \n",
        "        \n",
        "        return grad_weights, grad_biases\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "\n",
        "X = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "y_true = np.array([[1], [0]])\n",
        "weights = np.array([[0.5], [0.5]])\n",
        "biases = np.array([0.1])\n",
        "activations = np.array([[0.8], [0.6]])\n",
        "bp = Backpropagation()\n",
        "grad_weights, grad_biases = bp.compute(X, y_true, weights, biases, activations)\n",
        "print(\"Gradient of Loss w.r.t Weights:\", grad_weights)\n",
        "print(\"Gradient of Loss w.r.t Biases:\", grad_biases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiK3EeXEakO-"
      },
      "source": [
        "## **2.5 Training the Neural Network (3 missing blocks, 12 points in total)**\n",
        "\n",
        "Training involves:\n",
        "1. Performing a forward pass to compute predictions.\n",
        "2. Calculating the loss.\n",
        "3. Backpropagating gradients to update weights and biases.\n",
        "\n",
        "The update rule is:\n",
        "$W = W - \\eta \\cdot \\frac{\\partial L}{\\partial W}, \\quad b = b - \\eta \\cdot \\frac{\\partial L}{\\partial b}$\n",
        "\n",
        "### **Task**:\n",
        "Complete the training loop to:\n",
        "1. Perform forward passes.\n",
        "2. Compute the loss.\n",
        "3. Backpropagate gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "pGGXmdpYa1UO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Loss: 0.1770\n",
            "Epoch 20/100, Loss: 0.2238\n",
            "Epoch 30/100, Loss: 0.2704\n",
            "Epoch 40/100, Loss: 0.3132\n",
            "Epoch 50/100, Loss: 0.3501\n",
            "Epoch 60/100, Loss: 0.3804\n",
            "Epoch 70/100, Loss: 0.4044\n",
            "Epoch 80/100, Loss: 0.4229\n",
            "Epoch 90/100, Loss: 0.4368\n",
            "Epoch 100/100, Loss: 0.4472\n"
          ]
        }
      ],
      "source": [
        "class NeuralNetworkTrainer:\n",
        "    def __init__(self, perceptron, loss_function, backpropagation, learning_rate=0.01, epochs=100):\n",
        "        \"\"\"\n",
        "        Initialize the trainer.\n",
        "\n",
        "        Parameters:\n",
        "        - perceptron: Perceptron instance for forward passes.\n",
        "        - loss_function: BinaryCrossEntropy instance for loss computation.\n",
        "        - backpropagation: Backpropagation instance for gradient computation.\n",
        "        - learning_rate: Learning rate for gradient descent.\n",
        "        - epochs: Number of training iterations.\n",
        "        \"\"\"\n",
        "        self.perceptron = perceptron\n",
        "        self.loss_function = loss_function\n",
        "        self.backpropagation = backpropagation\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def train(self, X, y_true):\n",
        "        \"\"\"\n",
        "        Train the neural network using gradient descent.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features.\n",
        "        - y_true: Ground truth labels.\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epochs):\n",
        "            # Forward pass\n",
        "            #############################################################\n",
        "            # Your Turn: Use the Perceptron instance for forward passes\n",
        "            \n",
        "            forwardPass = self.perceptron.forward(X)\n",
        "\n",
        "            #\n",
        "            #############################################################\n",
        "\n",
        "            # Compute loss\n",
        "            #############################################################\n",
        "            # Your Turn: Use the BinaryCrossEntropy instance for loss computation\n",
        "            \n",
        "            loss = self.loss_function.compute(y_true, forwardPass)\n",
        "            #\n",
        "            #############################################################\n",
        "\n",
        "            # Backpropagation\n",
        "            #############################################################\n",
        "            # Your Turn: Use the Backpropagation instance to compute gradients\n",
        "            \n",
        "            grad_weights, grad_biases = self.backpropagation.compute(X, y_true, self.perceptron.weights, self.perceptron.bias, forwardPass)\n",
        "\n",
        "            #\n",
        "            #############################################################\n",
        "\n",
        "            # Update parameters\n",
        "            self.perceptron.weights -= self.learning_rate * grad_weights\n",
        "            self.perceptron.bias -= self.learning_rate * grad_biases\n",
        "\n",
        "            # Print loss every 10 epochs\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "# Initialize instances of the required classes\n",
        "weights = np.random.rand(3, 1)\n",
        "bias = np.random.rand(1)\n",
        "perceptron = Perceptron(weights, bias)\n",
        "loss_function = BinaryCrossEntropy()\n",
        "backpropagation = Backpropagation()\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = NeuralNetworkTrainer(perceptron, loss_function, backpropagation, learning_rate=0.01, epochs=100)\n",
        "\n",
        "# Training data\n",
        "X = np.random.rand(5, 3)\n",
        "y_true = np.array([[1], [0], [1], [1], [0]])\n",
        "\n",
        "# Train the model\n",
        "trainer.train(X, y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cbLo2pndTlv"
      },
      "source": [
        "# Part 3: Implementing RNNs from scratch (20 points)\n",
        "\n",
        "## Objective\n",
        "In this section, you will implement a basic Recurrent Neural Network (RNN) from scratch. You'll complete the following components:\n",
        "\n",
        "1. Initialize RNN parameters (5 points)\n",
        "2. Implement the forward pass for a single time step (5 points)\n",
        "3. Process a sequence through the RNN (5 points)\n",
        "4. Compute the loss and implement backpropagation (5 points)\n",
        "\n",
        "## Hints\n",
        "The equation for updating the hidden state is:\n",
        "\n",
        "$h_t = \\tanh(x_t \\cdot W_{xh} + h_{t-1} \\cdot W_{hh} + b_h)$\n",
        "\n",
        "The equation for computing the output is:\n",
        "\n",
        "$y_t = h_t \\cdot W_{hy} + b_y$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "Zpp4QhyCdTlv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.4.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srwh1_wzdTl2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize the RNN parameters.\n",
        "\n",
        "        Parameters:\n",
        "        - input_size: Size of input features\n",
        "        - hidden_size: Size of hidden state\n",
        "        - output_size: Size of output\n",
        "        \"\"\"\n",
        "        super(SimpleRNN, self).__init__()\n",
        "\n",
        "        #############################################################\n",
        "        # Your Turn: Initialize the network parameters\n",
        "        # Initialize:\n",
        "        \n",
        "        # - self.W_xh: Input to hidden weights\n",
        "        # - self.W_hh: Hidden to hidden weights\n",
        "        # - self.W_hy: Hidden to output weights\n",
        "        # - self.bh: Hidden bias\n",
        "        # - self.by: Output bias\n",
        "        # Use torch.randn() for initialization\n",
        "        \n",
        "        self.W_xh = torch.randn(input_size, hidden_size)\n",
        "        self.W_hh = torch.randn(hidden_size, hidden_size)  \n",
        "        self.W_hy = torch.randn(hidden_size, output_size) \n",
        "        self.bh = torch.zeros(hidden_size)\n",
        "        self.by = torch.zeros(output_size)\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "\n",
        "    def tanh(self, x):\n",
        "        \"\"\"Helper function: tanh activation\"\"\"\n",
        "        return torch.tanh(x)\n",
        "\n",
        "    def forward_step(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        Implement one step of the RNN.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input at current time step (batch_size, input_size)\n",
        "        - h_prev: Previous hidden state (batch_size, hidden_size)\n",
        "\n",
        "        Returns:\n",
        "        - h_next: Next hidden state\n",
        "        - y: Output at current time step\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: Implement the forward computation for a single time step\n",
        "        # Compute:\n",
        "        # 1. Next hidden state using tanh activation\n",
        "        # 2. Output for this time step\n",
        "        # Hint: h_next = tanh(W_xh @ x + W_hh @ h_prev + bh)\n",
        "        #       y = W_hy @ h_next + by\n",
        "        \n",
        "        h_next = self.tanh(x @ self.W_xh + h_prev @ self.W_hh  + self.bh)\n",
        "        \n",
        "        y =  h_next@ self.W_hy + self.by\n",
        "        \n",
        "        return h_next, y\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "    def forward(self, x_sequence):\n",
        "        \"\"\"\n",
        "        Process a sequence of inputs.\n",
        "\n",
        "        Parameters:\n",
        "        - x_sequence: Input sequence (seq_length, batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "        - outputs: Sequence of outputs\n",
        "        - hidden_states: Sequence of hidden states\n",
        "        \"\"\"\n",
        "        # Initialize lists to store outputs and hidden states\n",
        "        outputs = []\n",
        "        hidden_states = []\n",
        "        batch_size = x_sequence.shape[1]\n",
        "\n",
        "        # Initialize first hidden state with zeros\n",
        "        h_t = torch.zeros((batch_size, self.W_hh.shape[0]))\n",
        "\n",
        "        #############################################################\n",
        "        # Your Turn: Process sequence one step at a time\n",
        "        # Hint: Use a for loop to iterate over the sequence,\n",
        "        #       and call forward_step for each time step.\n",
        "        #       Store the results in hidden_states and outputs lists.\n",
        "        \n",
        "        for i in range(x_sequence.shape[0]):  \n",
        "            h_t, y_t = self.forward_step(x_sequence[i], h_t)  \n",
        "            outputs.append(y_t)\n",
        "            hidden_states.append(h_t)\n",
        "\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "        # Store results in lists and return\n",
        "        return torch.stack(outputs), torch.stack(hidden_states)\n",
        "\n",
        "def compute_loss_and_gradients(rnn, x_sequence, y_sequence):\n",
        "    \"\"\"\n",
        "    Compute loss and gradients for the RNN.\n",
        "\n",
        "    Parameters:\n",
        "    - rnn: SimpleRNN instance\n",
        "    - x_sequence: Input sequence\n",
        "    - y_sequence: Target sequence\n",
        "\n",
        "    Returns:\n",
        "    - loss: Mean squared error loss\n",
        "    - gradients: Dictionary of gradients for each parameter\n",
        "    \"\"\"\n",
        "    # Convert inputs to tensors and enable gradient tracking\n",
        "    x_sequence = x_sequence.clone().detach().requires_grad_(True)\n",
        "    y_sequence = y_sequence.clone().detach().requires_grad_(True)\n",
        "\n",
        "    # Zero any existing gradients\n",
        "    rnn.W_xh.requires_grad_(True)\n",
        "    rnn.W_hh.requires_grad_(True)\n",
        "    rnn.W_hy.requires_grad_(True)\n",
        "    rnn.bh.requires_grad_(True)\n",
        "    rnn.by.requires_grad_(True)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, hidden_states = rnn.forward(x_sequence)\n",
        "\n",
        "    #############################################################\n",
        "    # Your Turn: Compute MSE Loss\n",
        "    \n",
        "    loss = nn.MSELoss()(outputs,y_sequence)\n",
        "\n",
        "    #\n",
        "    #############################################################\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect gradients\n",
        "    gradients = {\n",
        "        'W_xh': rnn.W_xh.grad,\n",
        "        'W_hh': rnn.W_hh.grad,\n",
        "        'W_hy': rnn.W_hy.grad,\n",
        "        'bh': rnn.bh.grad,\n",
        "        'by': rnn.by.grad\n",
        "    }\n",
        "\n",
        "    return loss, gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S6NBcOidTl2"
      },
      "source": [
        "Run the following code to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOzxu4jEdTl3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([15, 32, 5])\n",
            "Hidden states shape: torch.Size([15, 32, 20])\n",
            "Loss: 15.73021125793457\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_size = 10\n",
        "hidden_size = 20\n",
        "output_size = 5\n",
        "batch_size = 32\n",
        "seq_length = 15\n",
        "\n",
        "# Create model instance\n",
        "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Generate sample data\n",
        "x_sequence = torch.randn(seq_length, batch_size, input_size)\n",
        "y_sequence = torch.randn(seq_length, batch_size, output_size)\n",
        "\n",
        "# Forward pass\n",
        "outputs, hidden_states = rnn(x_sequence)\n",
        "print(\"Output shape:\", outputs.shape)\n",
        "print(\"Hidden states shape:\", hidden_states.shape)\n",
        "\n",
        "# Compute loss and gradients\n",
        "loss, gradients = compute_loss_and_gradients(rnn, x_sequence, y_sequence)\n",
        "print(\"Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgJ9JUudTl3"
      },
      "source": [
        "# Declaration of Independent Work  \n",
        "\n",
        "I hereby declare that this assignment is entirely my own work and that I have neither given nor received unauthorized assistance in completing it. I have adhered to all the guidelines provided for this assignment and have cited all sources from which I derived data, ideas, or words, whether quoted directly or paraphrased.\n",
        "\n",
        "Furthermore, I understand that providing false declaration is a violation of the University of Arizona's honor code and will result in appropriate disciplinary action consistent with the severity of the violation.\n",
        "\n",
        "**Name:** Christian Ortmann\\\n",
        "**Date:** 02/21/2025"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
