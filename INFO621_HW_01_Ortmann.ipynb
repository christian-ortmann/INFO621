{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZoIQ97MtT0i"
      },
      "source": [
        "<h1 align=\"center\">INFO621 - Advanced Machine Learning Applications</h1>\n",
        "\n",
        "<h2 align=\"center\"><strong>Homework 1: Linear Models</strong></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NCMD1YptT0k"
      },
      "source": [
        "# Guidelines\n",
        "\n",
        "**Worth:** 10% of your final grade (**100 points total**)  \n",
        "**Submission Deadline:** Friday, February 7, 11:59 PM (Tucson Time)  \n",
        "\n",
        "\n",
        "### **Instructions**\n",
        "\n",
        "- For exercises involving code, write your solutions in the provided code chunks. You may add additional code chunks if needed.  \n",
        "- For exercises involving plots, ensure all axes and legends are labeled and give each plot an informative title.  \n",
        "- For exercises requiring descriptions or interpretations, use full sentences and provide clear, concise explanations.  \n",
        "\n",
        "### **Policies**\n",
        "\n",
        "**Sharing/Reusing Code Policy:**  \n",
        "You are allowed to use online resources (e.g., RStudio Community, StackOverflow) but **must explicitly cite** any external code you use or adapt. Failure to do so will be considered plagiarism, regardless of the source.  \n",
        "\n",
        "**Late Submission Policy:**  \n",
        "- **Less than 1 day late:** -25% of available points.  \n",
        "- **1-7 days late:** -50% of available points.  \n",
        "- **7 days or more late:** No credit will be awarded, and feedback will not be provided.  \n",
        "\n",
        "**Declaration of Independent Work:**  \n",
        "You must acknowledge your submission as your independent work by including your **name** and **date** at the end of the \"Declaration of Independent Work\" section.  \n",
        "\n",
        "### **Grading**\n",
        "\n",
        "- **Total Points:** 100 points.\n",
        "\n",
        "- **Grade Breakdown:**  \n",
        "  - **Part A (30 Points Total):**  \n",
        "    - Multiple-Choice Questions: 5 questions, 4 points each.  \n",
        "    - Descriptive Questions: 2 questions, 5 points each.  \n",
        "\n",
        "  - **Part B (30 Points Total):**  \n",
        "    - 3 code completion tasks, 10 points each.  \n",
        "\n",
        "  - **Part C (20 Points Total):**  \n",
        "    - 4 code completion tasks, 5 points each.  \n",
        "\n",
        "  - **Part 4 (20 Points Total):**  \n",
        "    - 3 code completion tasks. Task 1 & 2: 5 points each; Task 3: 10 points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u02om_-ztT0l"
      },
      "source": [
        "# Part 1. Quiz Questions (30 Points Total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEtM4qoVtT0l"
      },
      "source": [
        "- **Multiple-Choice Questions**: 5 questions, 4 points each  \n",
        "- **Descriptive Questions**: 2 questions, 5 points each"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C58eiwEtT0l"
      },
      "source": [
        "## 1.1 Multiple-Choice Questions (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nadh1MHWtT0m"
      },
      "source": [
        "1. **Consider a housing dataset for predicting prices using linear regression. You notice that the training error is low, but the test error is high. Which of the following is the LEAST likely cause?**  \n",
        "   - a) The model is overfitting due to high polynomial features.  \n",
        "   - b) The dataset contains noisy outliers that the model fits perfectly.  \n",
        "   - c) The training data is not representative of the test data.  \n",
        "   - d) The mean squared error was not minimized during training.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2beMv9O3wFoI"
      },
      "source": [
        "**Your Answer**: B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j2P0KhPtT0m"
      },
      "source": [
        "\n",
        "2. **Given a binary classification task where logistic regression is used, the sigmoid function outputs probabilities. What is a significant risk of setting the classification threshold to a value other than 0.5?**  \n",
        "   - a) Increased variance in the model's predictions.  \n",
        "   - b) Reduced ability to penalize large coefficients during training.  \n",
        "   - c) Misclassification of minority class due to an imbalanced dataset.  \n",
        "   - d) Increased computational complexity during training.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCPK702hwP0i"
      },
      "source": [
        "**Your Answer**: C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SfXmrbLtT0m"
      },
      "source": [
        "\n",
        "3. **You are applying Ridge regression to a dataset with 50 features. If the regularization parameter λ is set to a very high value, what is the MOST likely outcome?**  \n",
        "   - a) The model achieves perfect training accuracy.  \n",
        "   - b) Some feature weights are shrunk to exactly zero.  \n",
        "   - c) The model simplifies to predict the mean of the target variable.  \n",
        "   - d) All feature weights are adjusted uniformly but remain non-zero.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJYUY2ivwQwQ"
      },
      "source": [
        "**Your Answer**: C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNkalTz2tT0n"
      },
      "source": [
        "\n",
        "4. **A logistic regression model classifies emails as spam or not spam. Suppose the model is highly confident but consistently misclassifies emails containing certain rare phrases. What is the MOST plausible explanation?**  \n",
        "   - a) The sigmoid function is not correctly mapping probabilities.  \n",
        "   - b) The rare phrases are under-represented in the training data.  \n",
        "   - c) The regularization parameter λ is too low, causing overfitting.  \n",
        "   - d) Logistic regression cannot handle discrete data representations.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNnQuBbjwRH0"
      },
      "source": [
        "**Your Answer**: B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVQpaOLRtT0n"
      },
      "source": [
        "\n",
        "5. **Which of these scenarios would regularization likely fail to prevent overfitting?**  \n",
        "   - a) A large dataset with redundant features.\n",
        "   - b) A dataset with highly correlated features and outliers.\n",
        "   - c) A high-degree polynomial regression model on a small dataset.  \n",
        "   - d) A perfectly balanced dataset with equal classes and no noise.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29mbb1p8wRcy"
      },
      "source": [
        "**Your Answer**: C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv4hBm72tT0n"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWuZLf-stT0n"
      },
      "source": [
        "## 1.2  Descriptive Questions (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvvQE1OOtT0n"
      },
      "source": [
        "### Q1. Fraud Detection with Logistic Regression and Feature Selection  \n",
        "**Scenario:** Credit card fraud detection often involves datasets with a large number of features, some of which may be irrelevant or redundant. Feature selection can help simplify the model and improve its performance.  \n",
        "\n",
        "**Question:**  \n",
        "Explain how logistic regression can be combined with feature selection techniques, such as Lasso regression, to improve model performance. Why is feature selection especially important when working with high-dimensional datasets like those in fraud detection?  \n",
        "\n",
        "**Hint:**  \n",
        "Think about how Lasso regression penalizes less important features and forces some coefficients to zero. Consider the computational benefits and interpretability of the model when the number of features is reduced. Also, fraud datasets often have very few fraudulent cases compared to non-fraudulent ones. How would you ensure the model handles this class imbalance effectively?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kduc2H-0wSBq"
      },
      "source": [
        "**Your Answer**: Lasso logistic regression can be used as a feature selector to improve model performance by decreaseing computational cost and reducing the curse of dimensionality from having many feautures. This also decreases the risk that our model over fits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdIV1ailtT0o"
      },
      "source": [
        "### Q2. Validation Datasets and Bias-Variance Trade-Off  \n",
        "**Scenario:** The Week 2 lecture highlights the importance of validation datasets for model selection. However, a poorly chosen validation dataset can introduce issues in the model evaluation process.  \n",
        "\n",
        "**Question:**  \n",
        "What challenges arise if the validation dataset is not representative of the test dataset? Discuss how this can lead to problems with bias and variance in the model.  \n",
        "\n",
        "**Hint:**  \n",
        "Think about how a non-representative validation dataset might lead to a model that generalizes poorly. For example, an unbalanced validation set could result in overfitting or underfitting. Consider solutions such as cross-validation or ensuring the validation set is stratified and diverse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJvJOV_nwSc6"
      },
      "source": [
        "**Your Answer**: If the validation dataset is not presentative of the test dataset, then there is risk that the model will not be generalized enough because it is not tuning to data that is starkly different from the training data. This will lead to a model with low bias and high variance in its response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ8oMILyoKZ8"
      },
      "source": [
        "# Part 2. Implementing Linear Regression (30 Points Total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBNEueR_tT0o"
      },
      "source": [
        "- 3 code completion tasks, 10 points each"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piiVXuBIoSXL"
      },
      "source": [
        "## Objective\n",
        "In this part, you will implement parts of a simple linear regression model from scratch. You are required to complete the following tasks:\n",
        "1. Implement the `predict` function for the linear model.\n",
        "2. Calculate the Mean Squared Error (MSE) for model evaluation.\n",
        "3. Visualize the regression line along with the data points.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prdoLcPHoeiZ"
      },
      "outputs": [],
      "source": [
        "# Importing Necessary Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvvskg_3oZw1"
      },
      "source": [
        "## 2.1 Generating Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeK2WmwNoiBF"
      },
      "outputs": [],
      "source": [
        "# We will generate synthetic data for the living area size (X) and the housing price (Y).\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generating random living area sizes (in square feet)\n",
        "living_area = np.random.uniform(500, 3500, size=100)\n",
        "\n",
        "# Setting true parameters for the linear relationship\n",
        "true_slope = 0.3  # Slope in thousand dollars per square foot\n",
        "true_intercept = 50  # Intercept in thousand dollars\n",
        "\n",
        "# Generating housing prices with some noise\n",
        "noise = np.random.normal(10, 100, size=living_area.shape)  # Noise in thousand dollars\n",
        "housing_price = true_slope * living_area + true_intercept + noise\n",
        "\n",
        "# Visualizing the synthetic data\n",
        "plt.scatter(living_area, housing_price, alpha=0.7, label='Data points')\n",
        "plt.title(\"Synthetic Housing Price Data\")\n",
        "plt.xlabel(\"Living Area Size (sq ft)\")\n",
        "plt.ylabel(\"Housing Price ($1000)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtIUR2J4piC5"
      },
      "source": [
        "## 2.2 Linear Regression Model Implementation\n",
        "\n",
        "A simple linear regression model predicts $Y$ using the equation:\n",
        "$ Y = \\theta_0 + \\theta_1 X$\n",
        "Where:\n",
        "- $\\theta_0$ is the intercept.\n",
        "- $\\theta_1$ is the slope of the line.\n",
        "\n",
        "The following class, `LinearModel`, will be used to fit and evaluate the linear regression model.\n",
        "\n",
        "**Your Task 1: Implement the `predict` function for the linear model.**\n",
        "\n",
        "**Your Task 2: Calculate the Mean Squared Error (MSE) (`MSE` function) for model evaluation.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3wJs-99ptX-"
      },
      "outputs": [],
      "source": [
        "class LinearModel:\n",
        "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
        "        \"\"\"\n",
        "        Initialize the LinearModel class.\n",
        "\n",
        "        Parameters:\n",
        "        - learning_rate: Step size for gradient descent.\n",
        "        - iterations: Number of iterations for gradient descent.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterations = iterations\n",
        "        self.slope = None\n",
        "        self.intercept = None\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict using the trained linear model.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features (1D array).\n",
        "\n",
        "        Returns:\n",
        "        - Predicted values.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Task 1: Implement the `predict` function for the linear model.\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        \n",
        "        \n",
        "        #############################################################\n",
        "\n",
        "    def MSE(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate Mean Squared Error (MSE) loss.\n",
        "\n",
        "        Parameters:\n",
        "        - y_true: Actual values.\n",
        "        - y_pred: Predicted values.\n",
        "\n",
        "        Returns:\n",
        "        - mse: Mean Squared Error.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Task 2: Calculate the Mean Squared Error (MSE) (`MSE` function) for model evaluation.\n",
        "        # Your Turn: write your own code here\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "    # The `fit` function is used to train the linear regression model using gradient descent. Please do not modify this function.\n",
        "    def fit(self, X, y, print_interval=100):\n",
        "        \"\"\"\n",
        "        Fit the linear regression model using gradient descent.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features (1D array).\n",
        "        - y: Target values (1D array).\n",
        "        - print_interval: Frequency of printing the current MSE loss.\n",
        "        \"\"\"\n",
        "        # Initialize parameters\n",
        "        self.slope = 0.0\n",
        "        self.intercept = 0.0\n",
        "        n = len(X)\n",
        "\n",
        "        for i in range(self.iterations):\n",
        "            # Make predictions\n",
        "            y_pred = self.slope * X + self.intercept\n",
        "\n",
        "            # Calculate gradients\n",
        "            d_slope = (-2 / n) * np.sum(X * (y - y_pred))\n",
        "            d_intercept = (-2 / n) * np.sum(y - y_pred)\n",
        "\n",
        "            # Update parameters\n",
        "            self.slope -= self.learning_rate * d_slope\n",
        "            self.intercept -= self.learning_rate * d_intercept\n",
        "\n",
        "            # Print MSE loss at specified intervals\n",
        "            if (i + 1) % print_interval == 0:\n",
        "                mse = self.MSE(y, y_pred)\n",
        "                print(f\"Iteration {i + 1}/{self.iterations}, MSE: {mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9-FIcPStT0q"
      },
      "source": [
        "## 2.3 Model Training and Evaluation\n",
        "\n",
        "Now, let's train the model and visualize the results.\n",
        "\n",
        "**Your Task 3: Plot the regression line**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky3uC6TJtT0q"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model = LinearModel(learning_rate=0.0000001, iterations=10)\n",
        "model.fit(living_area, housing_price, print_interval=1)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(living_area)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(living_area, housing_price, alpha=0.7, label='Data points')\n",
        "\n",
        "#############################################################\n",
        "# Task 3: Plot the regression line\n",
        "# Your Turn: write your own code here\n",
        "#############################################################\n",
        "\n",
        "plt.title(\"Linear Regression Line\")\n",
        "plt.xlabel(\"Living Area Size (sq ft)\")\n",
        "plt.ylabel(\"Housing Price ($1000)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8pw55FYtT0q"
      },
      "source": [
        "---\n",
        "\n",
        "# **Part 3: House Price Modeling with sklearn (20 Points Total)**\n",
        "- 4 code completion tasks, 5 points each\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54AhZv0etT0q"
      },
      "source": [
        "This part demonstrates an **end-to-end** regression workflow. We will:\n",
        "\n",
        "1. **Load** the data from `train.csv`.\n",
        "2. **Remove outliers** in `GrLivArea`.\n",
        "3. Perform **basic feature engineering**:\n",
        "   - Round `BedroomAbvGr`\n",
        "   - Bin `YearBuilt` into decades (and one-hot-encode)\n",
        "   - Manually one-hot-encode `FullBath`, `HalfBath`\n",
        "4. **Compute correlation** with `SalePrice` and **filter** out features not meeting a threshold of \\(|r|\\ge 0.3\\).\n",
        "5. **Split** data into train/test sets.\n",
        "6. **Train and evaluate** multiple models:\n",
        "   - **LinearRegression**\n",
        "   - **RandomForestRegressor**\n",
        "   - **Ridge** (L2 regularized)\n",
        "   - **Lasso** (L1 regularized, with `max_iter` increased to avoid convergence issues)\n",
        "\n",
        "We will **evaluate** each model using **Mean Squared Error (MSE)** and the **R^2** score.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fnPdYg5WtT0q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/6n/cwl6lb1j5j54r8zxq47cqpbr0000gn/T/ipykernel_2656/344834359.py:2: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.stats import zscore\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z30wp7omtT0q"
      },
      "source": [
        "## 3.1 Load Data\n",
        "\n",
        "We'll use the Ames dataset from `train.csv`. Make sure the file is in the specified path relative to your notebook or adjust the `file_path` accordingly.\n",
        "\n",
        "The dataset is attached together with the homework notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O5F-4kvMtT0q"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>...</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
              "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
              "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
              "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
              "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
              "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
              "\n",
              "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
              "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
              "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
              "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
              "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
              "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
              "\n",
              "  YrSold  SaleType  SaleCondition  SalePrice  \n",
              "0   2008        WD         Normal     208500  \n",
              "1   2007        WD         Normal     181500  \n",
              "2   2008        WD         Normal     223500  \n",
              "3   2006        WD        Abnorml     140000  \n",
              "4   2008        WD         Normal     250000  \n",
              "\n",
              "[5 rows x 81 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path = \"621_data/train.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows to confirm loading\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj38R6JutT0q"
      },
      "source": [
        "Then, we need to define Features (X) and Target (y). We'll choose a subset of columns:\n",
        "\n",
        "- `GrLivArea`: Above-grade living area\n",
        "- `BedroomAbvGr`: Number of bedrooms above ground\n",
        "- `FullBath`: Number of full bathrooms\n",
        "- `HalfBath`: Number of half bathrooms\n",
        "- `YearBuilt`: Year the house was built\n",
        "\n",
        "We'll predict `SalePrice`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ozvmz0YdtT0q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Shape of X: (1460, 5)\n",
            "Initial Shape of y: (1460,)\n"
          ]
        }
      ],
      "source": [
        "X = df[['GrLivArea', 'BedroomAbvGr', 'FullBath', 'HalfBath', 'YearBuilt']]\n",
        "y = df['SalePrice']\n",
        "\n",
        "print(\"Initial Shape of X:\", X.shape)\n",
        "print(\"Initial Shape of y:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX8cC2_StT0q"
      },
      "source": [
        "## 3.2 Data Preprocessing & Feature Engineering\n",
        "\n",
        "We’ll remove outliers in `GrLivArea` using the Z-score method: keep rows where \\(|z| < 3\\).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBTEBbsJtT0q"
      },
      "outputs": [],
      "source": [
        "z_scores_grlivarea = zscore(X['GrLivArea'])\n",
        "outliers_mask = np.abs(z_scores_grlivarea) <= 3\n",
        "\n",
        "X = X[outliers_mask].copy()\n",
        "y = y[outliers_mask].copy()\n",
        "\n",
        "print(\"Shape of X after outlier removal:\", X.shape)\n",
        "print(\"Shape of y after outlier removal:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FawnCEFtT0r"
      },
      "source": [
        "And then, let's do some feature engineering:\n",
        "\n",
        "1. Round `BedroomAbvGr`: Some data might have non-integer values (rare, but possible). We round to ensure it’s an integer.\n",
        "\n",
        "2. Bin `YearBuilt` into decades: We'll create a categorical variable by binning the years into 10-year intervals.\n",
        "\n",
        "3. One-hot encode `FullBath` and `HalfBath` manually: Instead of a single integer column, we create separate columns for each unique value (e.g., `FullBath_0`, `FullBath_1`, etc.).\n",
        "Instead of a single integer column, we create separate columns for each unique value (e.g., `FullBath_0`, `FullBath_1`, etc.).\n",
        "\n",
        "4. One-hot encode the binned `YearBuilt`: We use `pd.get_dummies` for these categories. We set `drop_first=True` to avoid the dummy variable trap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "groCmJ8NtT0r"
      },
      "outputs": [],
      "source": [
        "# 5a. Round BedroomAbvGr\n",
        "X['BedroomAbvGr'] = X['BedroomAbvGr'].round()\n",
        "\n",
        "# 5b. Bin YearBuilt\n",
        "X['YearBuilt_Binned'] = pd.cut(\n",
        "    X['YearBuilt'],\n",
        "    bins=list(range(1880, 2030, 10)),  # e.g. 1880-1889, 1890-1899, etc.\n",
        "    labels=[f\"{decade}\" for decade in range(1880, 2020, 10)]\n",
        ")\n",
        "\n",
        "# 5c. One-hot encode FullBath and HalfBath\n",
        "for col in ['FullBath', 'HalfBath']:\n",
        "    unique_vals = X[col].unique()\n",
        "    for val in unique_vals:\n",
        "        X[f\"{col}_{val}\"] = (X[col] == val).astype(int)\n",
        "    X.drop(columns=[col], inplace=True)\n",
        "\n",
        "# 5d. One-hot encode binned YearBuilt\n",
        "X = pd.get_dummies(X, columns=['YearBuilt_Binned'], drop_first=True)\n",
        "\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1YyYqOotT0r"
      },
      "source": [
        "Then, we’ll compute the correlation of each feature in `X` with the target `y`, to see which features are strongly correlated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TPv9aDktT0r"
      },
      "outputs": [],
      "source": [
        "corr_with_target = X.corrwith(y)\n",
        "correlation_summary = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Correlation_with_SalePrice': corr_with_target\n",
        "}).dropna()  # drop any NaNs\n",
        "\n",
        "print(\"Full Correlation Summary:\")\n",
        "correlation_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYiqJqWitT0r"
      },
      "source": [
        "We’ll keep features where $|r|\\ge 0.3$. Adjust this threshold to keep more or fewer features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZXYGABOtT0r"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = 0.3\n",
        "keep_features = correlation_summary.loc[\n",
        "    correlation_summary['Correlation_with_SalePrice'].abs() >= THRESHOLD,\n",
        "    'Feature'\n",
        "]\n",
        "\n",
        "print(f\"Features kept (|r| >= {THRESHOLD}):\\n\", list(keep_features))\n",
        "\n",
        "# Subset X\n",
        "X = X[keep_features]\n",
        "print(\"New shape of X:\", X.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90y47o9RtT0u"
      },
      "source": [
        "We'll do an 70/30 train/test split using `train_test_split`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY708zZ3tT0u"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzUljtqNtT0u"
      },
      "source": [
        "## 3.3 Modeling\n",
        "\n",
        "We’ll fit and evaluate:\n",
        "\n",
        "1. **Linear Regression**\n",
        "2. **Random Forest** (via a pipeline with scaling)\n",
        "3. **Ridge Regression** (L2 regularization)\n",
        "4. **Lasso Regression** (L1 regularization, with scaling pipeline)\n",
        "\n",
        "We'll compare **Mean Squared Error (MSE)** and **R²** for each model.\n",
        "\n",
        "**Your Task 1: Initialize Linear Regression model**\n",
        "\n",
        "**Your Task 2: Initializing a Ridge regression model with a regularization parameter (alpha)**\n",
        "\n",
        "**Your Task 3: Add the Lasso regression model to the pipeline as a step**\n",
        "\n",
        "**Your Task 4: Use the pipeline to make predictions on the test data (X_test)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBMZ0KnitT0u"
      },
      "outputs": [],
      "source": [
        "# (a) Linear Regression\n",
        "\n",
        "#############################################################\n",
        "# Task 1: Initialize Linear Regression model\n",
        "# Your Turn: write your own code here\n",
        "\n",
        "#############################################################\n",
        "\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "print(\"--- Linear Regression ---\")\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred_lr))\n",
        "print(\"R2 :\", r2_score(y_test, y_pred_lr))\n",
        "\n",
        "\n",
        "# (b) Random Forest (quick example, no hyperparam tuning here)\n",
        "rf_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # optional for RF, but consistent pipeline\n",
        "    ('rf', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "y_pred_rf = rf_pipeline.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Random Forest ---\")\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred_rf))\n",
        "print(\"R2 :\", r2_score(y_test, y_pred_rf))\n",
        "\n",
        "\n",
        "# (c) Ridge Regression\n",
        "#############################################################\n",
        "# Task 2: initializing a Ridge regression model with a regularization parameter(alpha)\n",
        "# Your Turn: write your own code here\n",
        "\n",
        "#############################################################\n",
        "\n",
        "ridge_model.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Ridge Regression ---\")\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred_ridge))\n",
        "print(\"R2 :\", r2_score(y_test, y_pred_ridge))\n",
        "\n",
        "\n",
        "# (d) Lasso (with scaling)\n",
        "#############################################################\n",
        "# Task 3: Add the Lasso regression model to the pipeline as a step\n",
        "\n",
        "lasso_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    # Your Turn: Add the Lasso regression step here\n",
        "])\n",
        "#############################################################\n",
        "lasso_pipeline.fit(X_train, y_train)\n",
        "\n",
        "#############################################################\n",
        "# Task 4: Task: Use the pipeline to make predictions on the test data (X_test)\n",
        "# Your Turn: write your own code here\n",
        "\n",
        "#############################################################\n",
        "\n",
        "print(\"\\n--- Lasso Regression ---\")\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred_lasso))\n",
        "print(\"R2 :\", r2_score(y_test, y_pred_lasso))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuYxdetDtT0v"
      },
      "source": [
        "# Part 4. Implementing Logistic Regression by Hand  (20 Points Total)\n",
        "\n",
        "- 3 code completion tasks, Task 1 & 2: 5 point each; Task 3: 10 points.\n",
        "\n",
        "Let's do some simple hands-on exercise on _Logistic regression_.  \n",
        "\n",
        "We'll use a dataset from the popular [UCI dataset repository](https://archive.ics.uci.edu/ml/index.php); in particular, the [Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset. This is perhaps the best known database to be found in the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.\n",
        "\n",
        "We'll load in the data, take a look around and train a logistic regression classifier on it. We are going to implement logistic regression from scratch.\n",
        "\n",
        "Similar to linear regression, in logistic regression, input values ($\\mathbf{X}$) are combined linearly using weights or coefficient values to predict an output value ($\\mathbf{y}$). A key difference from linear regression is that the output value being modeled is a binary value ($0$ or $1$), rather than a numeric value.\n",
        "\n",
        "In logistic regression, we’re essentially trying to find the weights that **maximize the likelihood of the training data** $\\mathbf{X}$ and use them to categorize the target variable. Unlike linear regression, the likelihood maximization in logistic regression doesn’t have a closed form solution, and we'll need to solve the optimization problem with **gradient descent**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luk2wiavtT0v"
      },
      "source": [
        "## Objective\n",
        "\n",
        "In this part, you will implement parts of a simple logistic regression model from scratch. You are required to complete the following tasks:\n",
        "1. Implement the `sigmoid` function.\n",
        "2. Implement the `predict` function for the logistic regression model.\n",
        "3. Implement the objective function (log-likelihood) for the logistic regression model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpU7j1GAtT0v"
      },
      "source": [
        "## 4.1 Load the Iris dataset\n",
        "\n",
        "The Iris dataset is a built-in dataset in `sklearn`. We can load it directly from `sklearn.datasets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhR_5HmOtT0v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pWVc4aztT0v"
      },
      "outputs": [],
      "source": [
        "iris = datasets.load_iris() # Load the Iris dataset from sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVx6GPRQtT0v"
      },
      "source": [
        "By running the following code, we see what the dataset looks like.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGcfFUBmtT0v"
      },
      "outputs": [],
      "source": [
        "# Manufacture a dataframe for the raw data for sample inspection\n",
        "iris_pd = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])\n",
        "iris_pd.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9OW-i06tT0v"
      },
      "outputs": [],
      "source": [
        "# We can also take a look at some general statistics.\n",
        "iris_pd.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGqaNTSqtT0v"
      },
      "source": [
        "To simplify things, we take just the first two feature columns. Also, just to simplify things, we'll force the two non-linearly separable classes to be labeled with the same category, ending up with a **binary classification problem**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nqgdusRtT0v"
      },
      "outputs": [],
      "source": [
        "# Take the first two columns to populate a binary classification problem\n",
        "X_iris = iris.data[:, :2]\n",
        "# Collapse the other two non-zero class irises as one class\n",
        "Y_iris = (iris.target != 0) * 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jbKA7bEtT0v"
      },
      "source": [
        "Now each data point is a 2-dimensional vector, so we can easily visualize it by drawing a figure using the `plt` _matplotlib_ object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84RTHtcCtT0w"
      },
      "outputs": [],
      "source": [
        "# Produce a scatterplot for the values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_iris[Y_iris == 0][:, 0], X_iris[Y_iris == 0][:, 1], color='b', label='0')\n",
        "plt.scatter(X_iris[Y_iris == 1][:, 0], X_iris[Y_iris == 1][:, 1], color='r', label='1')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mImL4OCMtT0w"
      },
      "source": [
        "## 4.2 Implement the logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t85mRXPftT0w"
      },
      "source": [
        "### Implement the sigmoid function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9nzcReqtT0w"
      },
      "source": [
        "Before we dive into logistic regression, let’s take a look at the **logistic function** (or equivalently, sigmoid function), the heart of the logistic regression technique.  The logistic function is defined as:\n",
        "\n",
        "$$\n",
        "g(z) = \\frac{1}{1 + \\exp^{-z}}\n",
        "$$\n",
        "\n",
        "It is a \"S\"-shaped curve that maps any real value to the range between $0$ and $1$:\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png\" width=400 />\n",
        "</div>\n",
        "\n",
        " By Qef (The Standard Logistic Regression) [CC BY 4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.en), via Wikimedia Commons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f4oPNGStT0w"
      },
      "source": [
        "**Your task 1: Implement the `sigmoid` function.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P1lJeQYtT0w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Calculate the sigmoid function on an input\n",
        "\n",
        "    Args:\n",
        "        input (float): The input value to transform\n",
        "\n",
        "    Returns:\n",
        "        float: Transformed value for the input; bounded between 0.0 and 1.0\n",
        "    \"\"\"\n",
        "\n",
        "    #############################################################\n",
        "    # Task 1: Implement the `sigmoid` function.\n",
        "    # Your Turn: write your own code here\n",
        "    #\n",
        "    #############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AET8za04tT0w"
      },
      "source": [
        "You can run the following code to plot the figure of your sigmoid function, and check if your implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_58aq45dtT0w"
      },
      "outputs": [],
      "source": [
        "## Plotting testing harness\n",
        "%matplotlib inline\n",
        "\n",
        "# set the input space and define the function to plot\n",
        "x = np.linspace(-10, 10)\n",
        "y = sigmoid(x)\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# stylize the plot\n",
        "plt.style.use('ggplot')\n",
        "plt.xlim(-11,11)\n",
        "plt.ylim(0,1.1)\n",
        "\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.spines['bottom'].set_position(('data',0))\n",
        "ax.set_xticks([-10,-5,0,5,10])\n",
        "\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "ax.spines['left'].set_position(('data',0))\n",
        "ax.set_yticks([0,0.5,1])\n",
        "\n",
        "# perform and show the plot\n",
        "plt.plot(x,y,label=\"Sigmoid\",color = \"blue\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7lNiQTItT0w"
      },
      "source": [
        "## 4.3 Implement the prediction function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BpKXoe7tT0w"
      },
      "source": [
        "In summary, the prediction function of logistic regression is as follows:\n",
        "\n",
        "$$\n",
        "P(y = 1 | x) = h_{\\theta}(x) = \\text{sigmoid}(\\theta^\\top x) \\\\\n",
        "P(y = 0 | x) = 1 - P(y = 1 | x) = 1 - h_{\\theta}(x)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaECu8jntT0w"
      },
      "source": [
        "**Your task 2: Implement the `prediction` function of logistic regression by yourself.**\n",
        "\n",
        "Inputs:\n",
        "- `theta`: the weight vector $\\theta$ (an $n$-dimensional vector).\n",
        "- `X`: an $m \\times n$ matrix, where the $j$-th row is the $j$-th data point $x^{(j)}$.\n",
        "\n",
        "Returns: $h_{\\theta}(\\mathbf{X})$: A numpy array where the $j$-th entry is $h_{\\theta}(x^{(j)})$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSJt2HrYtT0w"
      },
      "outputs": [],
      "source": [
        "def prediction(theta, X):\n",
        "    \"\"\"Equivalent .predict() function in sklearn's classifier\n",
        "\n",
        "    Args:\n",
        "        theta (array of floats; n): The weight vector of our classifier\n",
        "        X (array of floats; m x n): The sample we want to test or make a prediction on\n",
        "\n",
        "    Returns:\n",
        "        array of floats; n: Prediction for the test_sample\n",
        "    \"\"\"\n",
        "\n",
        "    #############################################################\n",
        "    # Task 2: Implement the `prediction` function of logistic regression by yourself.\n",
        "    # Your Turn: write your own code here\n",
        "    #\n",
        "    #############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXIoNpGEtT0w"
      },
      "source": [
        "## 4.4 Calculating the Log-Likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bzE_zJttT0w"
      },
      "source": [
        "After we build up the logistic regression model, our goal is to search for a value of $\\theta$ so that the probability $P(y = 1 | x) = h_{\\theta}(x)$ is large when $x$ belongs to the $1$ class, and small when $x$ belongs to the $0$ class. We will learn $\\theta$ from the training data.\n",
        "\n",
        "For a set of training examples with binary labels $\\{(x^{(i)}, y^{(i)}) : i = 1, \\cdots , m \\}$, the log-likelihood of the training data measures how well our model fits the training data. The log-likelihood (denoted as $LL(\\theta)$) is calculated as follows (refer to the course lecture notes to see its derivation):\n",
        "\n",
        "$$\n",
        "J(\\theta) = - \\frac{1}{m} \\sum_i^m \\left(y^{(i)} \\log( h_\\theta(x^{(i)}) ) + (1 - y^{(i)}) \\log( 1 - h_\\theta(x^{(i)}) ) \\right).\n",
        "$$\n",
        "\n",
        "Note that only one of the two terms in the summation is non-zero, for each training example – depending on whether the label $y^{(i)}$ is $0$ or $1$. When $y^{(i)}=1$, minimizing the cost function implies that we need to make $h_{\\theta}(x^{(i)})$ large; and when $y^{(i)}=0$, we want to make $1−h_{\\theta}$ large, as explained above.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYgmHRS9tT0w"
      },
      "source": [
        "**Your task 3: Implement the `log_likelihood` function.**\n",
        "\n",
        "Inputs:\n",
        "- `X`: an $m \\times n$ matrix, where the $j$-th row is the feature vector of $x^{(j)}$.\n",
        "- `y`: a $m$-dimensional vector, where the $j$-th element is $y^{(j)}$.\n",
        "- `theta`: the weight vector $\\theta$ (a $n$-dimentional vector).\n",
        "\n",
        "Returns: $J(\\theta)$: A scalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ24tn1ttT0w"
      },
      "outputs": [],
      "source": [
        "def log_likelihood(X, y, theta):\n",
        "    \"\"\"Calculates the log likelihood of the\n",
        "\n",
        "    Args:\n",
        "        X (array of floats; m x n): The input data\n",
        "        y (array of floats; m): The target values for the data\n",
        "        theta (array of floats; n): The weight vector \\theta\n",
        "\n",
        "    Returns:\n",
        "        float: cost, the log likelihood of the data X\n",
        "    \"\"\"\n",
        "\n",
        "    #############################################################\n",
        "    # Task 3: Implement the `log_likelihood` function.\n",
        "    # Your Turn: write your own code here\n",
        "    #\n",
        "    #############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo6xHQZDtT0x"
      },
      "source": [
        "## 4.5 Training the logistic regression model\n",
        "\n",
        "We can now put all the parts together to implement the logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqDoQ1EUtT0x"
      },
      "outputs": [],
      "source": [
        "def gradient(X, y, theta):\n",
        "    \"\"\"Calculates the gradient of the log likelihood function\n",
        "\n",
        "    Args:\n",
        "        X (array of floats; m x n): The input data\n",
        "        y (array of floats; m): The target values for the data\n",
        "        theta (array of floats; n): The weight vector \\theta\n",
        "\n",
        "    Returns:\n",
        "        array of floats; n: The gradient of the log likelihood function\n",
        "    \"\"\"\n",
        "    predictions = prediction(theta, X)\n",
        "    gradient = np.dot(X.T, (predictions - y)) / float(len(y))\n",
        "    gradient = gradient.reshape(-1)\n",
        "    return gradient\n",
        "\n",
        "def logistic_regression(X, y, num_steps, learning_rate, verbose):\n",
        "    \"\"\"Optimises the weights for logistic regression, given a training dataset.\n",
        "\n",
        "    Args:\n",
        "        X (array of floats; m x n): The input data\n",
        "        y (array of floats; m): The target values for the data\n",
        "        num_steps (int): number of training iteration before termination\n",
        "        learning_rate (float): learning rate for the gradient descent\n",
        "        verbose (Boolean): print log-likelihood statistics?\n",
        "\n",
        "    Returns:\n",
        "        array of floats; n: The weight vector \\theta\n",
        "    \"\"\"\n",
        "    # Add the bias\n",
        "    bias = np.ones((X.shape[0], 1))\n",
        "    X = np.concatenate((bias, X), axis=1)\n",
        "\n",
        "    # Initialize the weights\n",
        "    weights = np.zeros(X.shape[1])\n",
        "\n",
        "    # Training with gradient descent\n",
        "    for step in range(num_steps):\n",
        "        # Calculate the gradient\n",
        "        grad = gradient(X, y, weights)\n",
        "        # Update the weights\n",
        "        weights = weights - learning_rate * grad\n",
        "        # Print log-likelihood every step\n",
        "        cost = log_likelihood(X, y, weights)\n",
        "        if verbose and step % 10000 == 0:\n",
        "            print('Number of iterations: {}; cost: {:.5f}'.format(step, cost))\n",
        "\n",
        "    return weights\n",
        "\n",
        "# Run the logistic regression model\n",
        "weights = logistic_regression(X_iris, Y_iris, num_steps = 300000, learning_rate = 0.1, verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwEiRQNQtT0x"
      },
      "source": [
        "## 4.6 Model evaluation\n",
        "\n",
        "Let's inspect what we've done. The following code prints the weights, classification accuracy, and plots the decision boundary.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxYthWxUtT0x"
      },
      "outputs": [],
      "source": [
        "def predict_prob(X, weights):\n",
        "    \"\"\"Returns prediction probabilities for the input data\n",
        "    Args:\n",
        "        X (array of floats; m x n): The input data\n",
        "        weights (array of floats; n): The weight vector \\theta\n",
        "\n",
        "    Returns:\n",
        "        array of floats; n: The predicted probability of +1 class, bounded (0,1)\n",
        "    \"\"\"\n",
        "    # Add the bias, x_0, for each example\n",
        "    bias = np.ones((X.shape[0], 1))\n",
        "    X = np.concatenate((bias, X), axis=1)\n",
        "\n",
        "    return sigmoid(np.dot(X, weights))\n",
        "\n",
        "print('The learned weights are {}'.format(weights))\n",
        "\n",
        "preds = predict_prob(X_iris, weights).round()\n",
        "# Calculate the accuracy\n",
        "accu = (preds == Y_iris).mean()\n",
        "print('The classification accuracy: {}'.format(accu))\n",
        "\n",
        "confidence = [0.5]\n",
        "boundary_colors = ['black']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_iris[Y_iris == 0][:, 0], X_iris[Y_iris == 0][:, 1], color='b', label='0')\n",
        "plt.scatter(X_iris[Y_iris == 1][:, 0], X_iris[Y_iris == 1][:, 1], color='r', label='1')\n",
        "plt.legend()\n",
        "x1_min, x1_max = X_iris[:,0].min(), X_iris[:,0].max(),\n",
        "x2_min, x2_max = X_iris[:,1].min(), X_iris[:,1].max(),\n",
        "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
        "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "probs = predict_prob(grid, weights).reshape(xx1.shape)\n",
        "plt.contour(xx1, xx2, probs, confidence, linewidths=1, colors=boundary_colors)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m12IvkHMtT0x"
      },
      "source": [
        "# Declaration of Independent Work  \n",
        "\n",
        "I hereby declare that this assignment is entirely my own work and that I have neither given nor received unauthorized assistance in completing it. I have adhered to all the guidelines provided for this assignment and have cited all sources from which I derived data, ideas, or words, whether quoted directly or paraphrased.\n",
        "\n",
        "Furthermore, I understand that providing false declaration is a violation of the University of Arizona's honor code and will result in appropriate disciplinary action consistent with the severity of the violation.\n",
        "\n",
        "**Name:** Christian J Ortmann \\\n",
        "**Date:** 02/07/2025  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
