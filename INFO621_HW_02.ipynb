{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RMRL3uydTlk"
      },
      "source": [
        "<h1 align=\"center\">INFO621 - Advanced Machine Learning Applications</h1>\n",
        "\n",
        "<h2 align=\"center\"><strong>Homework 2: Neural Language and Sequence Modeling</strong></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkQOtjDHdTln"
      },
      "source": [
        "# Guidelines\n",
        "\n",
        "**Worth:** 10% of your final grade (**100 points total**)  \n",
        "**Submission Deadline:** Friday, February 21, 11:59 PM (Tucson Time)  \n",
        "\n",
        "\n",
        "### **Instructions**\n",
        "\n",
        "- For exercises involving code, write your solutions in the provided code chunks. You may add additional code chunks if needed.  \n",
        "- For exercises involving plots, ensure all axes and legends are labeled and give each plot an informative title.  \n",
        "- For exercises requiring descriptions or interpretations, use full sentences and provide clear, concise explanations.  \n",
        "\n",
        "### **Policies**\n",
        "\n",
        "**Sharing/Reusing Code Policy:**  \n",
        "You are allowed to use online resources (e.g., RStudio Community, StackOverflow) but **must explicitly cite** any external code you use or adapt. Failure to do so will be considered plagiarism, regardless of the source.  \n",
        "\n",
        "**Late Submission Policy:**  \n",
        "- **Less than 1 day late:** -25% of available points.  \n",
        "- **1-7 days late:** -50% of available points.  \n",
        "- **7 days or more late:** No credit will be awarded, and feedback will not be provided.  \n",
        "\n",
        "**Declaration of Independent Work:**  \n",
        "You must acknowledge your submission as your independent work by including your **name** and **date** at the end of the \"Declaration of Independent Work\" section.  \n",
        "\n",
        "### **Grading**\n",
        "\n",
        "- **Total Points:** 100 points.\n",
        "\n",
        "- **Grade Breakdown:**  \n",
        "  - **Part 1 (40 Points Total):**  \n",
        "    - Multiple-Choice Questions: 5 questions, 4 points each.  \n",
        "    - Descriptive Questions: 4 questions, 5 points each.  \n",
        "\n",
        "  - **Part 2 (40 Points Total):**  \n",
        "    - 10 code completion tasks, 4 points each.  \n",
        "\n",
        "  - **Part 3 (20 Points Total):**  \n",
        "    - 4 code completion tasks, 5 points each.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zIcc3JNeeOC"
      },
      "source": [
        "# Part 1. Written Questions (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yupqEgrdTlo"
      },
      "source": [
        "- **Multiple-Choice Questions**: 5 questions, 4 points each  \n",
        "- **Descriptive Questions**: 4 questions, 5 points each"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMPMDe3HdTlp"
      },
      "source": [
        "## 1.1 Multiple-Choice Questions (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fRUVMbVdTlp"
      },
      "source": [
        "1. **What is the key role of the activation function in a perceptron? (5 points)**\n",
        "\n",
        "   - A) To initialize weights  \n",
        "   - B) To compute the weighted sum of inputs  \n",
        "   - C) To introduce non-linearity into the model\n",
        "   - D) To apply regularization  \n",
        "   - E) To update weights through gradient descent  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcsi6CeYdTlp"
      },
      "source": [
        "**Your Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OpdpgIWdTlq"
      },
      "source": [
        "2. **Which of the following is NOT a type of neural network activation function? (5 points)**\n",
        "\n",
        "   - A) Sigmoid  \n",
        "   - B) Tanh  \n",
        "   - C) ReLU  \n",
        "   - D) Softmax  \n",
        "   - E) Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDQVb3OsdTlq"
      },
      "source": [
        "**Your Answer**: C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY6lnPLadTlq"
      },
      "source": [
        "3. **What is backpropagation primarily used for in a neural network? (5 points)**\n",
        "\n",
        "   - A) Data normalization  \n",
        "   - B) Computing gradients for weight updates\n",
        "   - C) Generating predictions from the model  \n",
        "   - D) Preventing overfitting  \n",
        "   - E) Reducing the training time  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcKV6wP_dTlr"
      },
      "source": [
        "**Your Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vztxrqpgdTlr"
      },
      "source": [
        "4. **Which regularization method involves halting training at a point to avoid overfitting? (5 points)**\n",
        "\n",
        "   - A) Dropout  \n",
        "   - B) Batch Normalization  \n",
        "   - C) Weight Decay  \n",
        "   - D) Early Stopping\n",
        "   - E) Adaptive Learning  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apvsq-yWdTlr"
      },
      "source": [
        "**Your Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNpCLl_udTlr"
      },
      "source": [
        "5. **What is one major advantage of using mini-batch gradient descent over full-batch gradient descent? (5 points)**\n",
        "\n",
        "   - A) It avoids the vanishing gradient problem\n",
        "   - B) It improves the generalization of the model\n",
        "   - C) It balances computational efficiency and convergence stability\n",
        "   - D) It guarantees convergence to a global minimum  \n",
        "   - E) It completely eliminates overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47Tja0x4dTlr"
      },
      "source": [
        "**Your Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zclpWQNodTlr"
      },
      "source": [
        "## 1.2 Open-Ended Questions (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5JkM2YwdTlr"
      },
      "source": [
        "### Q1. Perceptron and XOR\n",
        "\n",
        "**Scenario**:\n",
        "Imagine you are tackling a binary classification task where the data exhibits a pattern similar to the XOR problem—a classic example where data points cannot be separated by a single straight line. This scenario illustrates the challenge of linear separability and sets the stage for understanding why a basic model might struggle with such tasks.\n",
        "\n",
        "**Question**:\n",
        "Why is a single-layer perceptron unable to solve the XOR problem, and how does adding multiple layers to a neural network overcome this limitation?\n",
        "\n",
        "**Hint**:\n",
        "Consider the concept of linear separability and how deeper networks can perform non-linear transformations.\n",
        "\n",
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPJOdMEMdTls"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmUGc4vmdTls"
      },
      "source": [
        "### Q2. Loss Functions: Binary Cross-Entropy vs. Mean Squared Error\n",
        "\n",
        "**Scenario**\n",
        "Imagine you are training a neural network to distinguish between two classes, such as identifying whether an email is spam or not. You need a reliable way to quantify how well your model's predictions match the actual outcomes. This scenario highlights the importance of selecting the right loss function based on the task at hand.  \n",
        "\n",
        "**Question**\n",
        "\n",
        "What is the purpose of the loss function in a neural network?**\n",
        "**How do binary cross-entropy (BCE) and mean squared error (MSE) differ in their application?\n",
        "\n",
        "**Hint**  \n",
        "Think about the types of tasks (**classification vs. regression**) each loss function is best suited for.\n",
        "\n",
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m5Mkx_FdTls"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PqFfdcfdTls"
      },
      "source": [
        "### Q3. Importance of Non-Linearity  \n",
        "\n",
        "**Scenario**\n",
        "\n",
        "Consider a scenario where you are building a **neural network** for a **complex task**, such as **recognizing handwritten digits**. The ability to distinguish between similar but distinct patterns hinges on the network’s capacity to model **non-linear relationships**. Without **non-linear transformations**, the network's power would be drastically limited.  \n",
        "\n",
        "**Question**  \n",
        "Why is non-linearity critical in neural networks, and what would be the consequence if the network's activation functions were purely linear?  \n",
        "\n",
        "**Hint**  \n",
        "Think about how **stacking linear layers** without **non-linear transformations** would affect the **model’s expressive power**.  \n",
        "\n",
        "**Answer**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEu81T8ddTls"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6t8ArA-dTls"
      },
      "source": [
        "### Q4. Dropout as a Regularization Method\n",
        "\n",
        "**Scenario**  \n",
        "Imagine training a **deep neural network** that performs **exceptionally well on the training data** but **fails to generalize to unseen data**—a common problem known as **overfitting**. To combat this, you decide to incorporate a technique that forces the network to learn **robust and redundant features**, thereby improving its **generalization ability**.  \n",
        "\n",
        "**Question**  \n",
        "What is dropout, and why is it particularly effective as a regularization method in deep neural networks?\n",
        "\n",
        "**Hint**  \n",
        "Consider how **dropout temporarily disables neurons during training** and what effect this has on the **network’s learning process**.  \n",
        "\n",
        "**Answer**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hxph1J2dTls"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIpX3ue3eY0m"
      },
      "source": [
        "# Part 2. Implementing a Simple Neural Network from Scratch (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymS1CJ51e9F2"
      },
      "source": [
        "## **Objective**\n",
        "In this homework, you will implement various components of a simple neural network from scratch. You are required to complete the following tasks:\n",
        "\n",
        "1. **Perceptron Basics**: Implement the forward pass of a perceptron, including computing the weighted sum and applying the sigmoid activation function.\n",
        "2. **Multi-Layer Perceptron**: Extend your implementation to include a two-layer neural network, using ReLU for the hidden layer and softmax for the output layer.\n",
        "3. **Loss Function**: Implement the binary cross-entropy loss function to evaluate the model's performance.\n",
        "4. **Backpropagation**: Compute the gradients of the loss with respect to the weights and biases using the backpropagation algorithm.\n",
        "5. **Training the Neural Network**: Train the neural network using gradient descent by iteratively updating the weights and biases to minimize the loss.\n",
        "\n",
        "By completing these tasks, you will gain hands-on experience in building and training neural networks and understanding their underlying mechanics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nroIjqHZesao"
      },
      "outputs": [],
      "source": [
        "# Necessary Imports\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmGNqqSBXKz1"
      },
      "source": [
        "## **2.1 Perceptron Basics (2 missing blocks, 8 points in total)**\n",
        "\n",
        "A perceptron calculates a weighted sum of inputs and applies an activation function to produce an output. The sigmoid activation function is defined as:\n",
        "\n",
        "$\\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}$\n",
        "\n",
        "### **Task**:\n",
        "1. Compute the weighted sum $z = x \\cdot w + b$, where $x$ is the input vector, $w$ is the weight vector, and $b$ is the bias term.\n",
        "2. Apply the sigmoid activation function to $z$ to compute the perceptron output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8pS0Br4yXQ7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: 0.8698915256370021\n"
          ]
        }
      ],
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, weights, bias):\n",
        "        \"\"\"\n",
        "        Initialize the Perceptron.\n",
        "\n",
        "        Parameters:\n",
        "        - weights: Array of weights for the input features.\n",
        "        - bias: Bias term.\n",
        "        \"\"\"\n",
        "        self.weights = np.array(weights)\n",
        "        self.bias = bias\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Sigmoid activation function.\n",
        "\n",
        "        Parameters:\n",
        "        - z: Weighted sum of inputs.\n",
        "\n",
        "        Returns:\n",
        "        - Sigmoid activation value.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        return 1/((1+np.exp(-z)))\n",
        "    \n",
        "\n",
        "        #############################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the perceptron.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input features.\n",
        "\n",
        "        Returns:\n",
        "        - Output after applying sigmoid activation.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        z = (np.dot(x,self.weights) + self.bias)\n",
        "        \n",
        "        return self.sigmoid(z)\n",
        "    \n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "\n",
        "# Example usage\n",
        "weights = [0.1, 0.2, 0.3]\n",
        "bias = 0.5\n",
        "perceptron = Perceptron(weights, bias)\n",
        "x = np.array([1.0, 2.0, 3.0])\n",
        "output = perceptron.forward(x)\n",
        "print(\"Output:\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYlL5cHXXawP"
      },
      "source": [
        "## **2.2 Multi-Layer Perceptron (3 missing blocks, 12 points in total)**\n",
        "\n",
        "A multi-layer perceptron (MLP) consists of layers of neurons, each performing a weighted sum followed by a non-linear activation function.\n",
        "\n",
        "The **ReLU** (Rectified Linear Unit) activation function is defined as:\n",
        "\n",
        "$\\text{ReLU}(z) = \\max(0, z)$\n",
        "\n",
        "The **softmax** function for the output layer is defined as:\n",
        "\n",
        "$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$\n",
        "\n",
        "### **Task**:\n",
        "1. Compute the hidden layer activations: $ z_1 = X \\cdot W_1 + b_1 $, $ a_1 = \\text{ReLU}(z_1) $.\n",
        "2. Compute the output layer activations: $ z_2 = a_1 \\cdot W_2 + b_2 $, $ a_2 = \\text{softmax}(z_2) $.\n",
        "3. Write the forward function of the Perceptron.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "43vO-0zgXiY7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output Activations: [[0.07614884 0.04654982]\n",
            " [0.2586917  0.15700724]\n",
            " [0.09902744 0.0706877 ]\n",
            " [0.10898894 0.06001826]\n",
            " [0.07072233 0.05215773]]\n"
          ]
        }
      ],
      "source": [
        "class MultiLayerPerceptron:\n",
        "    def __init__(self, W1, b1, W2, b2):\n",
        "        \"\"\"\n",
        "        Initialize the Multi-Layer Perceptron.\n",
        "\n",
        "        Parameters:\n",
        "        - W1: Weights for the hidden layer.\n",
        "        - b1: Biases for the hidden layer.\n",
        "        - W2: Weights for the output layer.\n",
        "        - b2: Biases for the output layer.\n",
        "        \"\"\"\n",
        "        self.W1 = np.array(W1)\n",
        "        self.b1 = np.array(b1)\n",
        "        self.W2 = np.array(W2)\n",
        "        self.b2 = np.array(b2)\n",
        "\n",
        "    def relu(self, z):\n",
        "        \"\"\"\n",
        "        ReLU activation function.\n",
        "\n",
        "        Parameters:\n",
        "        - z: Weighted sum of inputs.\n",
        "\n",
        "        Returns:\n",
        "        - ReLU activation value.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        return np.maximum(0,z)\n",
        "\n",
        "        # \n",
        "        #############################################################\n",
        "\n",
        "    def softmax(self, z):\n",
        "        \"\"\"\n",
        "        Softmax activation function.\n",
        "\n",
        "        Parameters:\n",
        "        - z: Weighted sum of inputs.\n",
        "\n",
        "        Returns:\n",
        "        - Softmax probabilities.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        return (np.exp(z))/(np.sum(np.exp(z)))\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass for the multi-layer perceptron.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features.\n",
        "\n",
        "        Returns:\n",
        "        - Output probabilities.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        \n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        z1 = np.dot(X,self.W1)+self.b1\n",
        "        a1 = self.relu(z1)\n",
        "        z2 = np.dot(a1,self.W2)+self.b2\n",
        "        a2 = self.softmax(z2)\n",
        "        \n",
        "        return a2\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "\n",
        "# Example usage\n",
        "W1 = np.random.rand(3, 4)\n",
        "b1 = np.random.rand(4)\n",
        "W2 = np.random.rand(4, 2)\n",
        "b2 = np.random.rand(2)\n",
        "X = np.random.rand(5, 3)\n",
        "mlp = MultiLayerPerceptron(W1, b1, W2, b2)\n",
        "output = mlp.forward(X)\n",
        "print(\"Output Activations:\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx8YJb4uZ8z6"
      },
      "source": [
        "## **2.3 Loss Function (1 missing blocks, 4 points in total)**\n",
        "\n",
        "The binary cross-entropy loss quantifies the error between predicted probabilities ($y_{pred}$) and actual labels ($y_{true}$). The formula is:\n",
        "\n",
        "$L = -\\left(y_{true} \\cdot \\log(y_{pred}) + (1 - y_{true}) \\cdot \\log(1 - y_{pred})\\right)$\n",
        "\n",
        "### **Task**:\n",
        "1. Compute the loss for each sample using the formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "z1Vf7zN2Z8TS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 2.3851790109107687\n"
          ]
        }
      ],
      "source": [
        "class BinaryCrossEntropy:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the BinaryCrossEntropy class.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def compute(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute binary cross-entropy loss.\n",
        "\n",
        "        Parameters:\n",
        "        - y_true: Ground truth labels.\n",
        "        - y_pred: Predicted probabilities.\n",
        "\n",
        "        Returns:\n",
        "        - Binary cross-entropy loss.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        return -(np.dot(y_true,np.log(y_pred))) + np.dot((1-y_true),(1-y_pred))\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "\n",
        "# Example usage\n",
        "y_true = np.array([1, 0, 1, 1, 0])\n",
        "y_pred = np.array([0.9, 0.2, 0.8, 0.7, 0.1])\n",
        "bce = BinaryCrossEntropy()\n",
        "loss = bce.compute(y_true, y_pred)\n",
        "print(\"Loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzDVIq98Xjv4"
      },
      "source": [
        "## **2.4 Backpropagation (1 missing blocks, 4 points in total)**\n",
        "\n",
        "In this task, you will compute:\n",
        "1. The gradient of the loss with respect to the outputs ($ \\frac{\\partial L}{\\partial \\text{output}} $).\n",
        "2. The gradients of the loss with respect to the weights ($ \\frac{\\partial L}{\\partial W} $) and biases ($ \\frac{\\partial L}{\\partial b} $) using the chain rule.\n",
        "\n",
        "### **Task**:\n",
        "Complete the `backpropagation` function by:\n",
        "1. Computing the gradient of the loss w.r.t. outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ifNvc27WakFw"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "shapes (2,1) and (2,1) not aligned: 1 (dim 1) != 2 (dim 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m activations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.8\u001b[39m], [\u001b[38;5;241m0.6\u001b[39m]])\n\u001b[1;32m     41\u001b[0m bp \u001b[38;5;241m=\u001b[39m Backpropagation()\n\u001b[0;32m---> 42\u001b[0m grad_weights, grad_biases \u001b[38;5;241m=\u001b[39m \u001b[43mbp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient of Loss w.r.t Weights:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grad_weights)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient of Loss w.r.t Biases:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grad_biases)\n",
            "Cell \u001b[0;32mIn[15], line 26\u001b[0m, in \u001b[0;36mBackpropagation.compute\u001b[0;34m(self, X, y_true, weights, biases, activations)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mCompute gradients for weights and biases.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m- grad_biases: Gradient of loss w.r.t biases.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#############################################################\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Your Turn: write your own code here\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m grad_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgradient(\u001b[43mBinaryCrossEntropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39mweights)\n\u001b[1;32m     28\u001b[0m grad_biases \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgradient(BinaryCrossEntropy()\u001b[38;5;241m.\u001b[39mcompute(y_true,activations)\u001b[38;5;241m/\u001b[39mbiases)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad_weights, grad_biases\n",
            "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36mBinaryCrossEntropy.compute\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mCompute binary cross-entropy loss.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m- Binary cross-entropy loss.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#############################################################\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Your Turn: write your own code here\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my_true),(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my_pred))\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (2,1) and (2,1) not aligned: 1 (dim 1) != 2 (dim 0)"
          ]
        }
      ],
      "source": [
        "class Backpropagation:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the Backpropagation class.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def compute(self, X, y_true, weights, biases, activations):\n",
        "        \"\"\"\n",
        "        Compute gradients for weights and biases.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data.\n",
        "        - y_true: Ground truth labels.\n",
        "        - weights: Weight matrix.\n",
        "        - biases: Bias vector.\n",
        "        - activations: Predicted outputs.\n",
        "\n",
        "        Returns:\n",
        "        - grad_weights: Gradient of loss w.r.t weights.\n",
        "        - grad_biases: Gradient of loss w.r.t biases.\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: write your own code here\n",
        "        \n",
        "        grad_weights = np.gradient(BinaryCrossEntropy().compute(y_true,activations)/weights)\n",
        "        \n",
        "        grad_biases = np.gradient(BinaryCrossEntropy().compute(y_true,activations)/biases)\n",
        "        \n",
        "        return grad_weights, grad_biases\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "\n",
        "X = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "y_true = np.array([[1], [0]])\n",
        "weights = np.array([[0.5], [0.5]])\n",
        "biases = np.array([0.1])\n",
        "activations = np.array([[0.8], [0.6]])\n",
        "bp = Backpropagation()\n",
        "grad_weights, grad_biases = bp.compute(X, y_true, weights, biases, activations)\n",
        "print(\"Gradient of Loss w.r.t Weights:\", grad_weights)\n",
        "print(\"Gradient of Loss w.r.t Biases:\", grad_biases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiK3EeXEakO-"
      },
      "source": [
        "## **2.5 Training the Neural Network (3 missing blocks, 12 points in total)**\n",
        "\n",
        "Training involves:\n",
        "1. Performing a forward pass to compute predictions.\n",
        "2. Calculating the loss.\n",
        "3. Backpropagating gradients to update weights and biases.\n",
        "\n",
        "The update rule is:\n",
        "$W = W - \\eta \\cdot \\frac{\\partial L}{\\partial W}, \\quad b = b - \\eta \\cdot \\frac{\\partial L}{\\partial b}$\n",
        "\n",
        "### **Task**:\n",
        "Complete the training loop to:\n",
        "1. Perform forward passes.\n",
        "2. Compute the loss.\n",
        "3. Backpropagate gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGGXmdpYa1UO"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkTrainer:\n",
        "    def __init__(self, perceptron, loss_function, backpropagation, learning_rate=0.01, epochs=100):\n",
        "        \"\"\"\n",
        "        Initialize the trainer.\n",
        "\n",
        "        Parameters:\n",
        "        - perceptron: Perceptron instance for forward passes.\n",
        "        - loss_function: BinaryCrossEntropy instance for loss computation.\n",
        "        - backpropagation: Backpropagation instance for gradient computation.\n",
        "        - learning_rate: Learning rate for gradient descent.\n",
        "        - epochs: Number of training iterations.\n",
        "        \"\"\"\n",
        "        self.perceptron = perceptron\n",
        "        self.loss_function = loss_function\n",
        "        self.backpropagation = backpropagation\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def train(self, X, y_true):\n",
        "        \"\"\"\n",
        "        Train the neural network using gradient descent.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features.\n",
        "        - y_true: Ground truth labels.\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epochs):\n",
        "            # Forward pass\n",
        "            #############################################################\n",
        "            # Your Turn: Use the Perceptron instance for forward passes\n",
        "            \n",
        "            Perceptron()\n",
        "\n",
        "            #\n",
        "            #############################################################\n",
        "\n",
        "            # Compute loss\n",
        "            #############################################################\n",
        "            # Your Turn: Use the BinaryCrossEntropy instance for loss computation\n",
        "\n",
        "            #\n",
        "            #############################################################\n",
        "\n",
        "            # Backpropagation\n",
        "            #############################################################\n",
        "            # Your Turn: Use the Backpropagation instance to compute gradients\n",
        "\n",
        "            #\n",
        "            #############################################################\n",
        "\n",
        "            # Update parameters\n",
        "            self.perceptron.weights -= self.learning_rate * grad_weights\n",
        "            self.perceptron.bias -= self.learning_rate * grad_biases\n",
        "\n",
        "            # Print loss every 10 epochs\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "# Initialize instances of the required classes\n",
        "weights = np.random.rand(3, 1)\n",
        "bias = np.random.rand(1)\n",
        "perceptron = Perceptron(weights, bias)\n",
        "loss_function = BinaryCrossEntropy()\n",
        "backpropagation = Backpropagation()\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = NeuralNetworkTrainer(perceptron, loss_function, backpropagation, learning_rate=0.01, epochs=100)\n",
        "\n",
        "# Training data\n",
        "X = np.random.rand(5, 3)\n",
        "y_true = np.array([[1], [0], [1], [1], [0]])\n",
        "\n",
        "# Train the model\n",
        "trainer.train(X, y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cbLo2pndTlv"
      },
      "source": [
        "# Part 3: Implementing RNNs from scratch (20 points)\n",
        "\n",
        "## Objective\n",
        "In this section, you will implement a basic Recurrent Neural Network (RNN) from scratch. You'll complete the following components:\n",
        "\n",
        "1. Initialize RNN parameters (5 points)\n",
        "2. Implement the forward pass for a single time step (5 points)\n",
        "3. Process a sequence through the RNN (5 points)\n",
        "4. Compute the loss and implement backpropagation (5 points)\n",
        "\n",
        "## Hints\n",
        "The equation for updating the hidden state is:\n",
        "\n",
        "$h_t = \\tanh(x_t \\cdot W_{xh} + h_{t-1} \\cdot W_{hh} + b_h)$\n",
        "\n",
        "The equation for computing the output is:\n",
        "\n",
        "$y_t = h_t \\cdot W_{hy} + b_y$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zpp4QhyCdTlv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.4.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Srwh1_wzdTl2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize the RNN parameters.\n",
        "\n",
        "        Parameters:\n",
        "        - input_size: Size of input features\n",
        "        - hidden_size: Size of hidden state\n",
        "        - output_size: Size of output\n",
        "        \"\"\"\n",
        "        super(SimpleRNN, self).__init__()\n",
        "\n",
        "        #############################################################\n",
        "        # Your Turn: Initialize the network parameters\n",
        "        # Initialize:\n",
        "        # - self.W_xh: Input to hidden weights\n",
        "        # - self.W_hh: Hidden to hidden weights\n",
        "        # - self.W_hy: Hidden to output weights\n",
        "        # - self.bh: Hidden bias\n",
        "        # - self.by: Output bias\n",
        "        # Use torch.randn() for initialization\n",
        "        \n",
        "        torch.randn()\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "\n",
        "    def tanh(self, x):\n",
        "        \"\"\"Helper function: tanh activation\"\"\"\n",
        "        return torch.tanh(x)\n",
        "\n",
        "    def forward_step(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        Implement one step of the RNN.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input at current time step (batch_size, input_size)\n",
        "        - h_prev: Previous hidden state (batch_size, hidden_size)\n",
        "\n",
        "        Returns:\n",
        "        - h_next: Next hidden state\n",
        "        - y: Output at current time step\n",
        "        \"\"\"\n",
        "        #############################################################\n",
        "        # Your Turn: Implement the forward computation for a single time step\n",
        "        # Compute:\n",
        "        # 1. Next hidden state using tanh activation\n",
        "        # 2. Output for this time step\n",
        "        # Hint: h_next = tanh(W_xh @ x + W_hh @ h_prev + bh)\n",
        "        #       y = W_hy @ h_next + by\n",
        "        \n",
        "        h_next = tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.bh)\n",
        "        \n",
        "        y = np.dot(self.W_hy,self.h_next) + self.by\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "    def forward(self, x_sequence):\n",
        "        \"\"\"\n",
        "        Process a sequence of inputs.\n",
        "\n",
        "        Parameters:\n",
        "        - x_sequence: Input sequence (seq_length, batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "        - outputs: Sequence of outputs\n",
        "        - hidden_states: Sequence of hidden states\n",
        "        \"\"\"\n",
        "        # Initialize lists to store outputs and hidden states\n",
        "        outputs = []\n",
        "        hidden_states = []\n",
        "        batch_size = x_sequence.shape[1]\n",
        "\n",
        "        # Initialize first hidden state with zeros\n",
        "        h_t = torch.zeros((batch_size, self.W_hh.shape[0]))\n",
        "\n",
        "        #############################################################\n",
        "        # Your Turn: Process sequence one step at a time\n",
        "        # Hint: Use a for loop to iterate over the sequence,\n",
        "        #       and call forward_step for each time step.\n",
        "        #       Store the results in hidden_states and outputs lists.\n",
        "        \n",
        "        for i in length(x_sequence): \n",
        "            x = self.forward_step(x_sequence[i])\n",
        "            inst_out = x.y\n",
        "            inst_hidden = x.h_next\n",
        "            outputs.append(inst_out)\n",
        "            outputs.append(inst_hidden)\n",
        "\n",
        "        #\n",
        "        #############################################################\n",
        "\n",
        "        # Store results in lists and return\n",
        "        return torch.stack(outputs), torch.stack(hidden_states)\n",
        "\n",
        "def compute_loss_and_gradients(rnn, x_sequence, y_sequence):\n",
        "    \"\"\"\n",
        "    Compute loss and gradients for the RNN.\n",
        "\n",
        "    Parameters:\n",
        "    - rnn: SimpleRNN instance\n",
        "    - x_sequence: Input sequence\n",
        "    - y_sequence: Target sequence\n",
        "\n",
        "    Returns:\n",
        "    - loss: Mean squared error loss\n",
        "    - gradients: Dictionary of gradients for each parameter\n",
        "    \"\"\"\n",
        "    # Convert inputs to tensors and enable gradient tracking\n",
        "    x_sequence = x_sequence.clone().detach().requires_grad_(True)\n",
        "    y_sequence = y_sequence.clone().detach().requires_grad_(True)\n",
        "\n",
        "    # Zero any existing gradients\n",
        "    rnn.W_xh.requires_grad_(True)\n",
        "    rnn.W_hh.requires_grad_(True)\n",
        "    rnn.W_hy.requires_grad_(True)\n",
        "    rnn.bh.requires_grad_(True)\n",
        "    rnn.by.requires_grad_(True)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, hidden_states = rnn.forward(x_sequence)\n",
        "\n",
        "    #############################################################\n",
        "    # Your Turn: Compute MSE Loss\n",
        "    \n",
        "    mse = (sum((y_true-y_pred)**2))/len(y_true)\n",
        "\n",
        "    #\n",
        "    #############################################################\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect gradients\n",
        "    gradients = {\n",
        "        'W_xh': rnn.W_xh.grad,\n",
        "        'W_hh': rnn.W_hh.grad,\n",
        "        'W_hy': rnn.W_hy.grad,\n",
        "        'bh': rnn.bh.grad,\n",
        "        'by': rnn.by.grad\n",
        "    }\n",
        "\n",
        "    return loss, gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S6NBcOidTl2"
      },
      "source": [
        "Run the following code to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "YOzxu4jEdTl3"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "randn() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.Generator generator, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[43], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create model instance\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m rnn \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleRNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Generate sample data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m x_sequence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(seq_length, batch_size, input_size)\n",
            "Cell \u001b[0;32mIn[42], line 27\u001b[0m, in \u001b[0;36mSimpleRNN.__init__\u001b[0;34m(self, input_size, hidden_size, output_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28msuper\u001b[39m(SimpleRNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#############################################################\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Your Turn: Initialize the network parameters\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# - self.by: Output bias\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Use torch.randn() for initialization\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: randn() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.Generator generator, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_size = 10\n",
        "hidden_size = 20\n",
        "output_size = 5\n",
        "batch_size = 32\n",
        "seq_length = 15\n",
        "\n",
        "# Create model instance\n",
        "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Generate sample data\n",
        "x_sequence = torch.randn(seq_length, batch_size, input_size)\n",
        "y_sequence = torch.randn(seq_length, batch_size, output_size)\n",
        "\n",
        "# Forward pass\n",
        "outputs, hidden_states = rnn(x_sequence)\n",
        "print(\"Output shape:\", outputs.shape)\n",
        "print(\"Hidden states shape:\", hidden_states.shape)\n",
        "\n",
        "# Compute loss and gradients\n",
        "loss, gradients = compute_loss_and_gradients(rnn, x_sequence, y_sequence)\n",
        "print(\"Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgJ9JUudTl3"
      },
      "source": [
        "# Declaration of Independent Work  \n",
        "\n",
        "I hereby declare that this assignment is entirely my own work and that I have neither given nor received unauthorized assistance in completing it. I have adhered to all the guidelines provided for this assignment and have cited all sources from which I derived data, ideas, or words, whether quoted directly or paraphrased.\n",
        "\n",
        "Furthermore, I understand that providing false declaration is a violation of the University of Arizona's honor code and will result in appropriate disciplinary action consistent with the severity of the violation.\n",
        "\n",
        "**Name:** ___________________________  \n",
        "**Date:** ___________________________  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
